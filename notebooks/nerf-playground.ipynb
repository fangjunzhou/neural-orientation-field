{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e115a31-a121-4d2e-82e3-5a11eb762be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "import pycolmap\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import neural_orientation_field.utils as utils\n",
    "import neural_orientation_field.colmap.colmap_utils as colutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354ec5df-70a4-4e13-81e7-fd73c33bfb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed rng for reproducability\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dab1fca-d995-408d-9c66-792cf1e90216",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535b31fe-7cc6-467d-aac2-3e818225ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Orientation Field config\n",
    "CONFIG_PATH = \"../nof-config.json\"\n",
    "COLMAP_MODEL_PATH = \"../data/cache/colmap/0/\"\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as config_file:\n",
    "    config_dict = json.load(config_file)\n",
    "project_config = utils.ProjectConfig.from_dict(config_dict)\n",
    "image_dir = project_config.input_path\n",
    "colmap_model_path = pathlib.Path(COLMAP_MODEL_PATH).resolve()\n",
    "\n",
    "image_dir, colmap_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814a9ba3-a47f-4d01-9c61-78a5c3d70bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COLMAP reconstruction\n",
    "colmap_model = pycolmap.Reconstruction(colmap_model_path)\n",
    "print(colmap_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4578a15f-4135-4954-ae40-d9e3500259a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = colmap_model.num_reg_images()\n",
    "cam_transforms, cam_params, image_file_names = colutils.get_camera_poses(colmap_model)\n",
    "num_images, cam_transforms.shape, cam_params.shape, len(image_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207c5384-d412-4450-a387-6f53024b0ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "# From view coordinate to world coordinate.\n",
    "cam_transforms_inv = []\n",
    "cam_origs = []\n",
    "cam_rays = []\n",
    "cam_focals = []\n",
    "for idx in range(num_images):\n",
    "    # Convert image to (h, w, 3) np.ndarray.\n",
    "    image = Image.open(image_dir / image_file_names[idx])\n",
    "    image = np.array(image)\n",
    "    images.append(image)\n",
    "    h, w, _ = image.shape\n",
    "    # Camera parameters.\n",
    "    f, cx, cy = cam_params[idx]\n",
    "    cam_focals.append(f)\n",
    "    # Camera pose.\n",
    "    cam_transform = cam_transforms[idx]\n",
    "    cam_transform_inv = np.linalg.inv(cam_transform)\n",
    "    cam_transforms_inv.append(cam_transform_inv)\n",
    "    # Calculate camera origins\n",
    "    cam_orig = np.matmul(cam_transform_inv, np.array([0, 0, 0, 1]))[:3]\n",
    "    cam_origs.append(cam_orig)\n",
    "    # Calculate camera ray.\n",
    "    pixel_coord = np.moveaxis(np.mgrid[0:h, 0:w], 0, -1) - np.array([cx, cy])\n",
    "    cam_ray_view = np.append(pixel_coord, f * np.ones((h, w, 1)), axis=2)\n",
    "    cam_ray_view_homo = np.append(cam_ray_view, np.zeros((h, w, 1)), axis=2)\n",
    "    cam_ray_world = np.matmul(\n",
    "        cam_transform_inv[np.newaxis, np.newaxis, :, :],\n",
    "        cam_ray_view_homo[:, :, :, np.newaxis]\n",
    "    ).reshape((h, w, -1))[:, :, :3]\n",
    "    cam_rays.append(cam_ray_world)\n",
    "\n",
    "# WARNING: This may fail if images are not captured by the same camera.\n",
    "images = np.array(images) / 256\n",
    "cam_transforms_inv = np.array(cam_transforms_inv)\n",
    "cam_origs = np.array(cam_origs)\n",
    "cam_rays = np.array(cam_rays) / np.linalg.norm(np.array(cam_rays), axis=-1)[:, :, :, np.newaxis]\n",
    "cam_focals = np.array(cam_focals)\n",
    "\n",
    "images.shape, cam_transforms.shape, cam_transforms_inv.shape, cam_origs.shape, cam_rays.shape, cam_focals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe84a06b-57c0-4f3e-a4a0-482ce212dfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "ax.scatter(cam_origs[:, 0], cam_origs[:, 1], cam_origs[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c341451a-91d6-45a5-85bc-a07dd4823e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "image = images[idx]\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68ee247-1992-4fc4-a09d-c8c208b31869",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"tiny_nerf_data\", images=images, poses=cam_transforms_inv, focal=cam_focals[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d082a6-e6dd-4dcf-a9ee-5566243b03fc",
   "metadata": {},
   "source": [
    "# NeRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2312eea-1e81-4a72-8f65-885683f4e841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MPS device.\n",
    "USE_DEVICE = \"cuda\"\n",
    "\n",
    "if USE_DEVICE == \"mps\" and torch.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif USE_DEVICE == \"cuda\" and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5152629f-f170-4a0d-a48d-e9baf45838f5",
   "metadata": {},
   "source": [
    "## Nerf Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bd76e8-967f-41e7-ac04-3707e0d0f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nerf model.\n",
    "class TinyNerfModel(torch.nn.Module):\n",
    "    def __init__(self, filter_size=128, num_encoding_functions=6):\n",
    "        super(TinyNerfModel, self).__init__()\n",
    "        # Input layer (default: 42 -> 128)\n",
    "        # 3 (position) + 3 (direction) + 3 * 2 * 6 (6 sin and cos encoding for position only)\n",
    "        self.layer1 = torch.nn.Linear(\n",
    "            3 * 2 + 3 * 2 * num_encoding_functions,\n",
    "            filter_size\n",
    "        )\n",
    "        # Layer 2 (default: 128 -> 128)\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(filter_size, filter_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(filter_size, filter_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(filter_size, filter_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(filter_size, filter_size),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        # Layer 3 (default: 128 -> 4)\n",
    "        self.layer3 = torch.nn.Linear(filter_size, 4)\n",
    "        # Short hand for torch.nn.functional.relu\n",
    "        self.relu = torch.nn.functional.relu\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "# Positional encoding.\n",
    "def pos_encode(x: torch.Tensor, num_encoding_functions=6):\n",
    "    \"\"\"\n",
    "    x: (n, 3) tensor of n positions.\n",
    "    return: (n, num_encoding_functions, 3) encoded positions\n",
    "    \"\"\"\n",
    "    p = torch.arange(num_encoding_functions, device=device)\n",
    "    x = torch.pow(2, p)[torch.newaxis, :, torch.newaxis] * x[:, torch.newaxis, :]\n",
    "    x = torch.concat((torch.sin(x), torch.cos(x)), 1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd15aa0-3b1f-4ba1-92b1-8e6a04eaf538",
   "metadata": {},
   "source": [
    "## Volumetric Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222e1dc1-2f49-4968-a975-6148d3d188fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volumetric rendering using NeRF model, camera_origs, and camera_rays\n",
    "def volumetric_rendering(\n",
    "    model: torch.nn.Module,\n",
    "    camera_origs: torch.Tensor,\n",
    "    camera_rays: torch.Tensor,\n",
    "    nc: torch.float32,\n",
    "    fc: torch.float32,\n",
    "    num_sample: int,\n",
    "    sample_jitter: float = 0.1,\n",
    "    num_pos_encode: int = 6,\n",
    "):\n",
    "    n, _ = camera_origs.shape\n",
    "    # Expand ray to multiple samples.\n",
    "    sample_depths = torch.linspace(nc, fc, num_sample, device=device)\n",
    "    sample_depths = sample_depths.unsqueeze(0).expand(n, -1)\n",
    "    # Improve convergence by introducing random sample.\n",
    "    interval = (fc - nc) / (num_sample - 1)\n",
    "    sample_depths = sample_depths + (torch.rand(sample_depths.shape, device=device) - 0.5) * interval * sample_jitter\n",
    "    sample_pos = camera_origs[:, torch.newaxis, :] + camera_rays[:, torch.newaxis, :] * sample_depths[:, :, torch.newaxis]\n",
    "    sample_direct = camera_rays.unsqueeze(1).expand((-1, num_sample, -1))\n",
    "\n",
    "    # Send samples to NeRF.\n",
    "    sample_pos = sample_pos.reshape(-1, 3)\n",
    "    sample_direct = sample_direct.reshape(-1, 3)\n",
    "    sample_pos_encode = pos_encode(sample_pos, num_pos_encode)\n",
    "\n",
    "    # Eval radiance from NeRF\n",
    "    nerf_input = torch.concat((sample_pos.unsqueeze(1), sample_direct.unsqueeze(1), sample_pos_encode), 1).reshape(n * num_sample, -1)\n",
    "    radiance = model(nerf_input)\n",
    "    radiance = radiance.reshape(n, num_sample, -1)\n",
    "\n",
    "    # Integrate color.\n",
    "    sample_depths_diff = sample_depths[:, 1:] - sample_depths[:, :-1]\n",
    "    sample_depths_diff = torch.concat((sample_depths_diff, torch.ones((n, 1), device=device) * interval), dim=1)\n",
    "    color = radiance[:, :, 0:3] * radiance[:, :, 3].unsqueeze(-1) * sample_depths_diff[:, :, torch.newaxis]\n",
    "    color = torch.sum(color, 1)\n",
    "    color = torch.sigmoid(color)\n",
    "    return color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f57e199-da2c-45a6-8e65-5c8e649f8801",
   "metadata": {},
   "source": [
    "# NeRF Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa452ae5-89c5-4add-8bd3-c7926e544a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Near/far clipping distance.\n",
    "nc = 1\n",
    "fc = 8\n",
    "# Positional encoding number.\n",
    "num_pos_encode = 8\n",
    "samples_per_ray = 32\n",
    "# Hyper parameters.\n",
    "lr = 5e-3\n",
    "num_iters = 1\n",
    "ray_batch_size = 5000\n",
    "\n",
    "# Init model.\n",
    "model = TinyNerfModel(num_encoding_functions=num_pos_encode)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2afa16-f823-4d88-abdb-a0b872a649cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show volumetric rendering.\n",
    "def render_nerf(idx = -1):\n",
    "    image_eval, cam_orig_eval, cam_ray_eval = images[idx], cam_origs[idx], cam_rays[idx]\n",
    "    image_eval = torch.from_numpy(image_eval).type(torch.float32).to(device)\n",
    "    cam_orig_eval = torch.from_numpy(cam_orig_eval).type(torch.float32).to(device)\n",
    "    cam_orig_eval = cam_orig_eval.view(1, 1, -1)\n",
    "    cam_orig_eval = cam_orig_eval.expand(image_eval.shape)\n",
    "    cam_ray_eval = torch.from_numpy(cam_ray_eval).type(torch.float32).to(device)\n",
    "    image_eval.shape, cam_orig_eval.shape, cam_ray_eval.shape\n",
    "    color_pred = volumetric_rendering(\n",
    "        model,\n",
    "        cam_orig_eval.reshape(-1, 3),\n",
    "        cam_ray_eval.reshape(-1, 3),\n",
    "        nc,\n",
    "        fc,\n",
    "        num_sample=samples_per_ray,\n",
    "        num_pos_encode=num_pos_encode\n",
    "    )\n",
    "    image_pred = color_pred.reshape(image_eval.shape)\n",
    "    return image_eval, image_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66191273-044e-4bd1-80b4-0a7aa7da80d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_eval, image_pred = render_nerf(1)\n",
    "fig, (ax_image, ax_pred) = plt.subplots(1, 2)\n",
    "ax_image.imshow(image_eval.cpu())\n",
    "ax_pred.imshow(image_pred.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e0fefb-b806-4cc3-8b51-f9d64eb86fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeRFDataset(Dataset):    \n",
    "    def __init__(self, images, cam_origs, cam_rays):\n",
    "        cam_origs = np.broadcast_to(cam_origs[:, np.newaxis, np.newaxis, :], images.shape)\n",
    "        self.colors = images.reshape(-1, 3)\n",
    "        self.cam_origs = cam_origs.reshape(-1, 3)\n",
    "        self.cam_rays = cam_rays.reshape(-1, 3)\n",
    "\n",
    "    # get sample\n",
    "    def __getitem__(self, idx):\n",
    "        return self.colors[idx], self.cam_origs[idx], self.cam_rays[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c544ea76-1672-4707-9a0d-a2c0e1313b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "dataset = NeRFDataset(images[:-1], cam_origs[:-1], cam_rays[:-1])\n",
    "train_sampler = RandomSampler(data_source=dataset, num_samples=int(1 * len(dataset)))\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    sampler=train_sampler,\n",
    "    batch_size=ray_batch_size,\n",
    ")\n",
    "\n",
    "writer = SummaryWriter(log_dir=project_config.cache_path / \"tensor-board\" / datetime.datetime.now().isoformat(), flush_secs=1)\n",
    "save_image_every_n_batch = 200\n",
    "for it in tqdm(range(num_iters)):\n",
    "    # One iteration of the training.\n",
    "    for batch_i, (color_batch, cam_orig_batch, cam_ray_batch) in enumerate(tqdm(dataloader)):\n",
    "        color_batch = color_batch.type(torch.float32).to(device)\n",
    "        cam_orig_batch = cam_orig_batch.type(torch.float32).to(device)\n",
    "        cam_ray_batch = cam_ray_batch.type(torch.float32).to(device)\n",
    "        \n",
    "        color_pred = volumetric_rendering(\n",
    "            model,\n",
    "            cam_orig_batch.reshape(-1, 3),\n",
    "            cam_ray_batch.reshape(-1, 3),\n",
    "            nc,\n",
    "            fc,\n",
    "            num_sample=samples_per_ray,\n",
    "            num_pos_encode=num_pos_encode\n",
    "        )\n",
    "        loss = torch.nn.functional.mse_loss(color_pred, color_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        writer.add_scalar(\"MSE Loss\", loss, it * len(dataloader) + batch_i)\n",
    "        if batch_i % save_image_every_n_batch == 0:\n",
    "            model.eval()\n",
    "            image_eval, image_pred = render_nerf(-1)\n",
    "            writer.add_image(\"Eval Image\", image_eval, it * len(dataloader) + batch_i, dataformats=\"HWC\")\n",
    "            writer.add_image(\"Pred Image\", image_pred, it * len(dataloader) + batch_i, dataformats=\"HWC\")\n",
    "            model.train()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f3888-b90d-4aa7-95c1-5609f0a6d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_eval, image_pred = render_nerf(0)\n",
    "fig, (ax_image, ax_pred) = plt.subplots(1, 2)\n",
    "ax_image.imshow(image_eval.cpu())\n",
    "ax_pred.imshow(image_pred.detach().cpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
