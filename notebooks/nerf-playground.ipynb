{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e115a31-a121-4d2e-82e3-5a11eb762be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "import pycolmap\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import neural_orientation_field.utils as utils\n",
    "import neural_orientation_field.colmap.colmap_utils as colutils\n",
    "import neural_orientation_field.nerf.dataset as nfdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "354ec5df-70a4-4e13-81e7-fd73c33bfb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10fc15210>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seed rng for reproducability\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dab1fca-d995-408d-9c66-792cf1e90216",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "535b31fe-7cc6-467d-aac2-3e818225ea11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/Users/fangjun/Documents/stanford/cs229/final-project/data/images/hoover-tower'),\n",
       " PosixPath('/Users/fangjun/Documents/stanford/cs229/final-project/data/output/colmap/model/0'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Neural Orientation Field config\n",
    "IMAGE_PATH = \"../data/images/hoover-tower/\"\n",
    "COLMAP_MODEL_PATH = \"../data/output/colmap/model/0/\"\n",
    "\n",
    "image_dir = pathlib.Path(IMAGE_PATH).resolve()\n",
    "colmap_model_path = pathlib.Path(COLMAP_MODEL_PATH).resolve()\n",
    "\n",
    "image_dir, colmap_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "814a9ba3-a47f-4d01-9c61-78a5c3d70bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1024, 1024, 3), (), (4, 4), (4, 4), (3,), (1024, 1024, 3))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataset = nfdata.NeRFImageDataset(image_dir, colmap_model_path)\n",
    "image, f, cam_transform, cam_transform_inv, cam_orig, cam_ray_world = image_dataset[0]\n",
    "image.shape, f.shape, cam_transform.shape, cam_transform_inv.shape, cam_orig.shape, cam_ray_world.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e67b898-5096-4196-a04c-cc4d3177c0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371ab26649c24ff58a42fe7c308341c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Image:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tqdm(total=len(image_dataset), desc=\"Processing Image\") as progress:\n",
    "    ray_dataset = nfdata.NeRFRayDataset(image_dataset, progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aff749ef-a1c5-4d2f-ae62-7e52594d615c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((3,), (3,), (3,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig, direc, pixel = ray_dataset[0]\n",
    "orig.shape, direc.shape, pixel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "947ed66a-613c-4484-bed7-6824a4618fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "127\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3145727 is out of bounds for axis 0 with size 1048576",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ray_dataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(ray_dataset)\n\u001b[0;32m----> 2\u001b[0m ray_dataset[\u001b[38;5;241m0\u001b[39m], \u001b[43mray_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mray_dataset_size\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/Documents/stanford/cs229/final-project/src/neural_orientation_field/nerf/dataset.py:86\u001b[0m, in \u001b[0;36mNeRFRayDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(image_idx)\n\u001b[1;32m     85\u001b[0m pixel_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefix_idx[image_idx]\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcam_origs[image_idx], \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcam_ray_worlds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpixel_idx\u001b[49m\u001b[43m]\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpixels[image_idx][pixel_idx]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3145727 is out of bounds for axis 0 with size 1048576"
     ]
    }
   ],
   "source": [
    "ray_dataset_size = len(ray_dataset)\n",
    "ray_dataset[0], ray_dataset[ray_dataset_size-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a98cc96-b10d-450d-96e7-dc044aff87b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray_dataset_size/1024**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe84a06b-57c0-4f3e-a4a0-482ce212dfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "ax.scatter(cam_origs[:, 0], cam_origs[:, 1], cam_origs[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c341451a-91d6-45a5-85bc-a07dd4823e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "image, _, _, _, _, _ = image_dataset[idx]\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d082a6-e6dd-4dcf-a9ee-5566243b03fc",
   "metadata": {},
   "source": [
    "# NeRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2312eea-1e81-4a72-8f65-885683f4e841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MPS device.\n",
    "USE_DEVICE = \"cuda\"\n",
    "\n",
    "if USE_DEVICE == \"mps\" and torch.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif USE_DEVICE == \"cuda\" and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5152629f-f170-4a0d-a48d-e9baf45838f5",
   "metadata": {},
   "source": [
    "## Nerf Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bd76e8-967f-41e7-ac04-3707e0d0f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nerf model.\n",
    "class TinyNerfModel(torch.nn.Module):\n",
    "    def __init__(self, filter_size=128, num_encoding_functions=6):\n",
    "        super(TinyNerfModel, self).__init__()\n",
    "        # Input layer (default: 42 -> 128)\n",
    "        # 3 (position) + 3 (direction) + 3 * 2 * 6 (6 sin and cos encoding for position only)\n",
    "        self.layer1 = torch.nn.Linear(\n",
    "            3 * 2 + 3 * 2 * num_encoding_functions,\n",
    "            filter_size\n",
    "        )\n",
    "        # Layer 2 (default: 128 -> 128)\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(filter_size, filter_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(filter_size, filter_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(filter_size, filter_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(filter_size, filter_size),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        # Layer 3 (default: 128 -> 4)\n",
    "        self.layer3 = torch.nn.Linear(filter_size, 4)\n",
    "        # Short hand for torch.nn.functional.relu\n",
    "        self.relu = torch.nn.functional.relu\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "# Positional encoding.\n",
    "def pos_encode(x: torch.Tensor, num_encoding_functions=6):\n",
    "    \"\"\"\n",
    "    x: (n, 3) tensor of n positions.\n",
    "    return: (n, num_encoding_functions, 3) encoded positions\n",
    "    \"\"\"\n",
    "    p = torch.arange(num_encoding_functions, device=device)\n",
    "    x = torch.pow(2, p)[torch.newaxis, :, torch.newaxis] * x[:, torch.newaxis, :]\n",
    "    x = torch.concat((torch.sin(x), torch.cos(x)), 1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd15aa0-3b1f-4ba1-92b1-8e6a04eaf538",
   "metadata": {},
   "source": [
    "## Volumetric Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222e1dc1-2f49-4968-a975-6148d3d188fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volumetric rendering using NeRF model, camera_origs, and camera_rays\n",
    "def volumetric_rendering(\n",
    "    model: torch.nn.Module,\n",
    "    camera_origs: torch.Tensor,\n",
    "    camera_rays: torch.Tensor,\n",
    "    nc: torch.float32,\n",
    "    fc: torch.float32,\n",
    "    num_sample: int,\n",
    "    sample_jitter: float = 0.1,\n",
    "    num_pos_encode: int = 6,\n",
    "):\n",
    "    n, _ = camera_origs.shape\n",
    "    # Expand ray to multiple samples.\n",
    "    sample_depths = torch.linspace(nc, fc, num_sample, device=device)\n",
    "    sample_depths = sample_depths.unsqueeze(0).expand(n, -1)\n",
    "    # Improve convergence by introducing random sample.\n",
    "    interval = (fc - nc) / (num_sample - 1)\n",
    "    sample_depths = sample_depths + (torch.rand(sample_depths.shape, device=device) - 0.5) * interval * sample_jitter\n",
    "    sample_pos = camera_origs[:, torch.newaxis, :] + camera_rays[:, torch.newaxis, :] * sample_depths[:, :, torch.newaxis]\n",
    "    sample_direct = camera_rays.unsqueeze(1).expand((-1, num_sample, -1))\n",
    "\n",
    "    # Send samples to NeRF.\n",
    "    sample_pos = sample_pos.reshape(-1, 3)\n",
    "    sample_direct = sample_direct.reshape(-1, 3)\n",
    "    sample_pos_encode = pos_encode(sample_pos, num_pos_encode)\n",
    "\n",
    "    # Eval radiance from NeRF\n",
    "    nerf_input = torch.concat((sample_pos.unsqueeze(1), sample_direct.unsqueeze(1), sample_pos_encode), 1).reshape(n * num_sample, -1)\n",
    "    radiance = model(nerf_input)\n",
    "    radiance = radiance.reshape(n, num_sample, -1)\n",
    "\n",
    "    # Integrate color.\n",
    "    sample_depths_diff = sample_depths[:, 1:] - sample_depths[:, :-1]\n",
    "    sample_depths_diff = torch.concat((sample_depths_diff, torch.ones((n, 1), device=device) * interval), dim=1)\n",
    "    color = radiance[:, :, 0:3] * radiance[:, :, 3].unsqueeze(-1) * sample_depths_diff[:, :, torch.newaxis]\n",
    "    color = torch.sum(color, 1)\n",
    "    color = torch.sigmoid(color)\n",
    "    return color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f57e199-da2c-45a6-8e65-5c8e649f8801",
   "metadata": {},
   "source": [
    "# NeRF Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa452ae5-89c5-4add-8bd3-c7926e544a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Near/far clipping distance.\n",
    "nc = 1\n",
    "fc = 8\n",
    "# Positional encoding number.\n",
    "num_pos_encode = 8\n",
    "samples_per_ray = 32\n",
    "# Hyper parameters.\n",
    "lr = 5e-3\n",
    "num_iters = 8\n",
    "ray_batch_size = 5000\n",
    "\n",
    "# Init model.\n",
    "model = TinyNerfModel(num_encoding_functions=num_pos_encode)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2afa16-f823-4d88-abdb-a0b872a649cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show volumetric rendering.\n",
    "def render_nerf(idx = -1):\n",
    "    image_eval, cam_orig_eval, cam_ray_eval = images[idx], cam_origs[idx], cam_rays[idx]\n",
    "    image_eval = torch.from_numpy(image_eval).type(torch.float32).to(device)\n",
    "    cam_orig_eval = torch.from_numpy(cam_orig_eval).type(torch.float32).to(device)\n",
    "    cam_orig_eval = cam_orig_eval.view(1, 1, -1)\n",
    "    cam_orig_eval = cam_orig_eval.expand(image_eval.shape)\n",
    "    cam_ray_eval = torch.from_numpy(cam_ray_eval).type(torch.float32).to(device)\n",
    "    image_eval.shape, cam_orig_eval.shape, cam_ray_eval.shape\n",
    "    color_pred = volumetric_rendering(\n",
    "        model,\n",
    "        cam_orig_eval.reshape(-1, 3),\n",
    "        cam_ray_eval.reshape(-1, 3),\n",
    "        nc,\n",
    "        fc,\n",
    "        num_sample=samples_per_ray,\n",
    "        num_pos_encode=num_pos_encode\n",
    "    )\n",
    "    image_pred = color_pred.reshape(image_eval.shape)\n",
    "    return image_eval, image_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66191273-044e-4bd1-80b4-0a7aa7da80d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_eval, image_pred = render_nerf(1)\n",
    "fig, (ax_image, ax_pred) = plt.subplots(1, 2)\n",
    "ax_image.imshow(image_eval.cpu())\n",
    "ax_pred.imshow(image_pred.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e0fefb-b806-4cc3-8b51-f9d64eb86fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeRFDataset(Dataset):    \n",
    "    def __init__(self, images, cam_origs, cam_rays):\n",
    "        cam_origs = np.broadcast_to(cam_origs[:, np.newaxis, np.newaxis, :], images.shape)\n",
    "        self.colors = images.reshape(-1, 3)\n",
    "        self.cam_origs = cam_origs.reshape(-1, 3)\n",
    "        self.cam_rays = cam_rays.reshape(-1, 3)\n",
    "\n",
    "    # get sample\n",
    "    def __getitem__(self, idx):\n",
    "        return self.colors[idx], self.cam_origs[idx], self.cam_rays[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c544ea76-1672-4707-9a0d-a2c0e1313b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "dataset = NeRFDataset(images[:-1], cam_origs[:-1], cam_rays[:-1])\n",
    "train_sampler = RandomSampler(data_source=dataset, num_samples=int(1 * len(dataset)))\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    sampler=train_sampler,\n",
    "    batch_size=ray_batch_size,\n",
    ")\n",
    "\n",
    "writer = SummaryWriter(log_dir=project_config.cache_path / \"tensor-board\" / datetime.datetime.now().isoformat(), flush_secs=1)\n",
    "save_image_every_n_batch = 200\n",
    "for it in tqdm(range(num_iters)):\n",
    "    # One iteration of the training.\n",
    "    for batch_i, (color_batch, cam_orig_batch, cam_ray_batch) in enumerate(tqdm(dataloader)):\n",
    "        color_batch = color_batch.type(torch.float32).to(device)\n",
    "        cam_orig_batch = cam_orig_batch.type(torch.float32).to(device)\n",
    "        cam_ray_batch = cam_ray_batch.type(torch.float32).to(device)\n",
    "        \n",
    "        color_pred = volumetric_rendering(\n",
    "            model,\n",
    "            cam_orig_batch.reshape(-1, 3),\n",
    "            cam_ray_batch.reshape(-1, 3),\n",
    "            nc,\n",
    "            fc,\n",
    "            num_sample=samples_per_ray,\n",
    "            num_pos_encode=num_pos_encode\n",
    "        )\n",
    "        loss = torch.nn.functional.mse_loss(color_pred, color_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        writer.add_scalar(\"MSE Loss\", loss, it * len(dataloader) + batch_i)\n",
    "        if batch_i % save_image_every_n_batch == 0:\n",
    "            model.eval()\n",
    "            image_eval, image_pred = render_nerf(-1)\n",
    "            writer.add_image(\"Eval Image\", image_eval, it * len(dataloader) + batch_i, dataformats=\"HWC\")\n",
    "            writer.add_image(\"Pred Image\", image_pred, it * len(dataloader) + batch_i, dataformats=\"HWC\")\n",
    "            model.train()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f3888-b90d-4aa7-95c1-5609f0a6d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_eval, image_pred = render_nerf(0)\n",
    "fig, (ax_image, ax_pred) = plt.subplots(1, 2)\n",
    "ax_image.imshow(image_eval.cpu())\n",
    "ax_pred.imshow(image_pred.detach().cpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
