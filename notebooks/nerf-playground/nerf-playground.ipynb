{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e115a31-a121-4d2e-82e3-5a11eb762be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "import pycolmap\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import RandomSampler, DataLoader, random_split\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import pyvista as pv\n",
    "\n",
    "import neural_orientation_field.utils as utils\n",
    "import neural_orientation_field.colmap.colmap_utils as colutils\n",
    "from neural_orientation_field.nerf.dataset import NeRFImageDataset, NeRFPriorImageDataset, NeRFRayDataset\n",
    "from neural_orientation_field.nerf.model import NeRfCoarseModel, NeRfFineModel\n",
    "from neural_orientation_field.nerf.utils import pos_encode, static_volumetric_renderer, adaptive_volumetric_renderer, cam_ray_from_pose, nerf_image_render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354ec5df-70a4-4e13-81e7-fd73c33bfb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed rng for reproducability\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dab1fca-d995-408d-9c66-792cf1e90216",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535b31fe-7cc6-467d-aac2-3e818225ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "CHECKPOINT_PATH = \"../../data/output/nerf/model/\"\n",
    "EVAL_OUTPUT_PATH = \"../../data/output/nerf/eval/\"\n",
    "\n",
    "checkpoint_path = pathlib.Path(CHECKPOINT_PATH).resolve()\n",
    "if not checkpoint_path.exists():\n",
    "    checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
    "eval_path = pathlib.Path(EVAL_OUTPUT_PATH).resolve()\n",
    "if not eval_path.exists():\n",
    "    eval_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "checkpoint_path, eval_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe4caa6-a466-4126-8933-150932547389",
   "metadata": {},
   "source": [
    "## COLMAP Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5231ddc3-19c0-4ceb-bd07-6e87c4aadc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Input\n",
    "# IMAGE_PATH = \"../../data/images/hoover-tower-mid-res/\"\n",
    "# COLMAP_MODEL_PATH = \"../../data/output/colmap/hoover-tower-mid-res/model/0/\"\n",
    "# \n",
    "# image_dir = pathlib.Path(IMAGE_PATH).resolve()\n",
    "# colmap_model_path = pathlib.Path(COLMAP_MODEL_PATH).resolve()\n",
    "# \n",
    "# image_dir, colmap_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814a9ba3-a47f-4d01-9c61-78a5c3d70bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_dataset = NeRFImageDataset(image_dir, colmap_model_path)\n",
    "# num_train = int(0.9 * len(image_dataset))\n",
    "# num_test = len(image_dataset) - num_train\n",
    "# image_dataset_train, image_dataset_test = random_split(image_dataset, [num_train, num_test])\n",
    "# len(image_dataset_train), len(image_dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faae5cdc-022b-47a5-9cd4-77352cda08fc",
   "metadata": {},
   "source": [
    "## Blender Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c692271-b08d-45c4-81f9-d6fdf54f9c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "IMAGE_PATH = \"../../data/images/blender-hair-long/\"\n",
    "CAMERA_PATH = \"../../data/camera/blender-hair-long/\"\n",
    "\n",
    "image_path = pathlib.Path(IMAGE_PATH).resolve()\n",
    "camera_path = pathlib.Path(CAMERA_PATH).resolve()\n",
    "\n",
    "frame_name_path = camera_path / \"frame-names.txt\"\n",
    "cam_transform_path = camera_path / \"camera-transforms.npy\"\n",
    "cam_param_path = camera_path / \"camera-params.npy\"\n",
    "\n",
    "image_path, frame_name_path, cam_transform_path, cam_param_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e86b953-8e60-4858-a4cc-3cb6445032a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blender Image Dataset\n",
    "with open(frame_name_path, \"r\") as frame_path_file:\n",
    "    frame_names = frame_path_file.read().split(\"\\n\")\n",
    "    frame_paths = [image_path / frame_name for frame_name in frame_names]\n",
    "with open(cam_transform_path, \"rb\") as cam_transform_file:\n",
    "    cam_transforms = np.load(cam_transform_file)\n",
    "with open(cam_param_path, \"rb\") as cam_param_file:\n",
    "    cam_params = np.load(cam_param_file)\n",
    "image_dataset = NeRFPriorImageDataset(frame_paths, cam_params, cam_transforms)\n",
    "num_train = int(0.9 * len(image_dataset))\n",
    "num_test = len(image_dataset) - num_train\n",
    "image_dataset_train, image_dataset_test = random_split(image_dataset, [num_train, num_test])\n",
    "len(image_dataset_train), len(image_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e67b898-5096-4196-a04c-cc4d3177c0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=len(image_dataset_train), desc=\"Processing Image\") as progress:\n",
    "    ray_dataset = NeRFRayDataset(image_dataset_train, progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c341451a-91d6-45a5-85bc-a07dd4823e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "image, cam_transform, (h, w), (f, cx, cy) = image_dataset[idx]\n",
    "plt.imshow(image)\n",
    "cam_orig, cam_ray_world = cam_ray_from_pose(cam_transform, h, w, f, cx, cy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc23bb-8e0f-4447-9ac8-ecf7dcd4d681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize camera rays.\n",
    "# origins = np.tile(cam_orig[np.newaxis, np.newaxis, :], (image.shape[0], image.shape[1], 1)).reshape(-1, 3)\n",
    "# directions = cam_ray_world.reshape(-1, 3)\n",
    "# colors = image.reshape(-1, 3)\n",
    "# sample_idxs = np.arange(0, colors.shape[0])\n",
    "# np.random.shuffle(sample_idxs)\n",
    "# sample_idxs = sample_idxs[:1024]\n",
    "# origins = origins[sample_idxs]\n",
    "# directions = directions[sample_idxs]\n",
    "# colors = colors[sample_idxs]\n",
    "\n",
    "# origins = []\n",
    "# directions = []\n",
    "# colors = []\n",
    "# for i in range(65535):\n",
    "#     idx = random.randint(0, len(ray_dataset))\n",
    "#     origin, direction, color = ray_dataset[idx]\n",
    "#     origins.append(origin)\n",
    "#     directions.append(direction)\n",
    "#     colors.append(color)\n",
    "# origins = np.array(origins)\n",
    "# directions = np.array(directions)\n",
    "# colors = np.array(colors)\n",
    "# \n",
    "# # Normalize directions if necessary\n",
    "# directions = directions / np.linalg.norm(directions, axis=1)[:, np.newaxis]\n",
    "# \n",
    "# # Create the PyVista PolyData object\n",
    "# vectors = pv.PolyData(origins)\n",
    "# \n",
    "# # Add the directions and colors as point data\n",
    "# vectors[\"directions\"] = directions\n",
    "# vectors[\"colors\"] = colors\n",
    "# \n",
    "# # Create the glyphs (arrows)\n",
    "# arrows = vectors.glyph(orient=\"directions\", scale=False, factor=1)\n",
    "# \n",
    "# # Plot the arrows with colors\n",
    "# plotter = pv.Plotter()\n",
    "# plotter.add_mesh(arrows, scalars=\"colors\", rgb=True)\n",
    "# plotter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d082a6-e6dd-4dcf-a9ee-5566243b03fc",
   "metadata": {},
   "source": [
    "# NeRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2312eea-1e81-4a72-8f65-885683f4e841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MPS device.\n",
    "USE_DEVICE = \"mps\"\n",
    "\n",
    "if USE_DEVICE == \"mps\" and torch.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif USE_DEVICE == \"cuda\" and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f57e199-da2c-45a6-8e65-5c8e649f8801",
   "metadata": {},
   "source": [
    "# NeRF Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa452ae5-89c5-4add-8bd3-c7926e544a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Near/far clipping distance.\n",
    "nc = 1\n",
    "fc = 8\n",
    "# Positional encoding number.\n",
    "coarse_pos_encode = 4\n",
    "fine_pos_encode = 6\n",
    "samples_per_ray = 4\n",
    "max_subd_samples = 4\n",
    "# Hyper parameters.\n",
    "lr = 2e-4\n",
    "num_iters = 4\n",
    "ray_batch_size = 8192\n",
    "# Training settings.\n",
    "size_train_ray = 1\n",
    "save_image_every_n_batch = 128\n",
    "\n",
    "# Init model.\n",
    "coarse_model = NeRfCoarseModel(num_encoding_functions=coarse_pos_encode)\n",
    "coarse_model.to(device)\n",
    "coarse_optimizer = torch.optim.Adam(coarse_model.parameters(), lr=lr)\n",
    "\n",
    "fine_model = NeRfFineModel(num_encoding_functions=fine_pos_encode)\n",
    "fine_model.to(device)\n",
    "fine_optimizer = torch.optim.Adam(fine_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66191273-044e-4bd1-80b4-0a7aa7da80d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test rendering.\n",
    "image_eval, cam_transform, (h, w), (f, cx, cy) = image_dataset_test[0]\n",
    "cam_orig, cam_ray_world = cam_ray_from_pose(cam_transform, h, w, f, cx, cy)\n",
    "coarse_model.eval()\n",
    "_, image_pred = nerf_image_render(\n",
    "    coarse_model,\n",
    "    fine_model,\n",
    "    cam_orig,\n",
    "    cam_ray_world,\n",
    "    ray_batch_size,\n",
    "    nc,\n",
    "    fc,\n",
    "    samples_per_ray,\n",
    "    max_subd_samples,\n",
    "    coarse_pos_encode,\n",
    "    fine_pos_encode,\n",
    "    device\n",
    ")\n",
    "fig, (ax_image, ax_pred) = plt.subplots(1, 2)\n",
    "ax_image.imshow(image_eval)\n",
    "ax_pred.imshow(image_pred.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c544ea76-1672-4707-9a0d-a2c0e1313b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_model.train()\n",
    "fine_model.train()\n",
    "\n",
    "train_sampler = RandomSampler(data_source=ray_dataset, num_samples=int(size_train_ray * len(ray_dataset)))\n",
    "dataloader = DataLoader(\n",
    "    ray_dataset,\n",
    "    sampler=train_sampler,\n",
    "    batch_size=ray_batch_size,\n",
    ")\n",
    "\n",
    "writer = SummaryWriter(flush_secs=1)\n",
    "test_image_idx = 0\n",
    "test_image, _, _, _ = image_dataset_test[test_image_idx]\n",
    "writer.add_image(\"Test Image Ground Truth\", test_image, dataformats=\"HWC\")\n",
    "for it in tqdm(range(num_iters)):\n",
    "    # One iteration of the training.\n",
    "    for batch_i, (cam_orig_batch, cam_ray_batch, color_batch) in enumerate(tqdm(dataloader)):\n",
    "        cam_orig_batch = cam_orig_batch.type(torch.float32).to(device)\n",
    "        cam_ray_batch = cam_ray_batch.type(torch.float32).to(device)\n",
    "        color_batch = color_batch.type(torch.float32).to(device)\n",
    "        coarse_color_pred, occupancy, sample_depth = static_volumetric_renderer(\n",
    "            coarse_model,\n",
    "            cam_orig_batch.reshape(-1, 3),\n",
    "            cam_ray_batch.reshape(-1, 3),\n",
    "            nc,\n",
    "            fc,\n",
    "            num_sample=samples_per_ray,\n",
    "            num_pos_encode=coarse_pos_encode,\n",
    "            device=device\n",
    "        )\n",
    "        loss_coarse = torch.nn.functional.mse_loss(coarse_color_pred, color_batch)\n",
    "        fine_color_pred, _, _ = adaptive_volumetric_renderer(\n",
    "            fine_model,\n",
    "            cam_orig_batch.reshape(-1, 3),\n",
    "            cam_ray_batch.reshape(-1, 3),\n",
    "            occupancy,\n",
    "            sample_depth,\n",
    "            max_subd_sample=max_subd_samples,\n",
    "            num_pos_encode=fine_pos_encode,\n",
    "            device=device\n",
    "        )\n",
    "        loss_fine = torch.nn.functional.mse_loss(fine_color_pred, color_batch)\n",
    "        loss = loss_coarse + loss_fine\n",
    "        loss.backward()\n",
    "        coarse_optimizer.step()\n",
    "        coarse_optimizer.zero_grad()\n",
    "        fine_optimizer.step()\n",
    "        fine_optimizer.zero_grad()\n",
    "        writer.add_scalar(\"Coarse Loss\", loss_coarse, (it * len(dataloader) + batch_i) * ray_batch_size)\n",
    "        writer.add_scalar(\"Fine Loss\", loss_fine, (it * len(dataloader) + batch_i) * ray_batch_size)\n",
    "        if batch_i % save_image_every_n_batch == 0:\n",
    "            coarse_model.eval()\n",
    "            fine_model.eval()\n",
    "            _, cam_transform, (h, w), (f, cx, cy) = image_dataset_test[0]\n",
    "            cam_orig, cam_ray_world = cam_ray_from_pose(cam_transform, h, w, f, cx, cy)\n",
    "            coarse_pred, fine_pred = nerf_image_render(\n",
    "                coarse_model,\n",
    "                fine_model,\n",
    "                cam_orig,\n",
    "                cam_ray_world,\n",
    "                ray_batch_size,\n",
    "                nc,\n",
    "                fc,\n",
    "                samples_per_ray,\n",
    "                max_subd_samples,\n",
    "                coarse_pos_encode,\n",
    "                fine_pos_encode,\n",
    "                device\n",
    "            )\n",
    "            writer.add_image(\"Rendered Test Image Fine\", fine_pred, (it *\n",
    "                             len(dataloader) + batch_i) * ray_batch_size, dataformats=\"HWC\")\n",
    "            writer.add_image(\"Rendered Test Image Coarse\", coarse_pred, (it *\n",
    "                             len(dataloader) + batch_i) * ray_batch_size, dataformats=\"HWC\")\n",
    "            coarse_model.train()\n",
    "            fine_model.train()\n",
    "    torch.save(coarse_model.state_dict(), checkpoint_path / f\"coarse_epoch_{it}.pth\")\n",
    "    torch.save(fine_model.state_dict(), checkpoint_path / f\"fine_epoch_{it}.pth\")\n",
    "writer.close()\n",
    "\n",
    "torch.save(coarse_model.state_dict(), checkpoint_path / f\"coarse_final.pth\")\n",
    "torch.save(fine_model.state_dict(), checkpoint_path / f\"fine_final.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5338ce8-a7e6-44dc-8825-b92e56f512cc",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c253aba-a3b7-40c3-a0fe-1a5db2c3e770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model.\n",
    "coarse_model_path = checkpoint_path / f\"coarse_final.pth\"\n",
    "coarse_model = NeRfCoarseModel(num_encoding_functions=coarse_pos_encode)\n",
    "coarse_model.load_state_dict(torch.load(coarse_model_path))\n",
    "coarse_model.to(device)\n",
    "fine_model_path = checkpoint_path / f\"fine_final.pth\"\n",
    "fine_model = NeRfFineModel(num_encoding_functions=fine_pos_encode)\n",
    "fine_model.load_state_dict(torch.load(fine_model_path))\n",
    "fine_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f3888-b90d-4aa7-95c1-5609f0a6d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval rendering.\n",
    "coarse_model.eval()\n",
    "fine_model.eval()\n",
    "for idx, (image_eval, cam_transform, (h, w), (f, cx, cy)) in enumerate(tqdm(image_dataset_test)):\n",
    "    cam_orig, cam_ray_world = cam_ray_from_pose(cam_transform, h, w, f, cx, cy)\n",
    "    _, image_pred = nerf_image_render(\n",
    "        coarse_model,\n",
    "        fine_model,\n",
    "        cam_orig,\n",
    "        cam_ray_world,\n",
    "        ray_batch_size,\n",
    "        nc,\n",
    "        fc,\n",
    "        samples_per_ray,\n",
    "        max_subd_samples,\n",
    "        coarse_pos_encode,\n",
    "        fine_pos_encode,\n",
    "        device\n",
    "    )\n",
    "    image_eval = (image_eval * 255).astype(np.uint8)\n",
    "    image_pred = (image_pred.detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "    image = Image.fromarray(image_eval)\n",
    "    image.save(eval_path / f\"test_{idx}_ground_truth.jpg\")\n",
    "    image = Image.fromarray(image_pred)\n",
    "    image.save(eval_path / f\"test_{idx}_pred.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
