{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d1395dc-caa0-4d80-a77c-10c2742bedf2",
   "metadata": {},
   "source": [
    "# NeRF Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c40ec712-c19e-46f3-950d-c3f273414ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from neural_orientation_field.nerf.dataset import NeRFPriorImageDataset\n",
    "from neural_orientation_field.nerf.model import NeRfCoarseModel, NeRfFineModel\n",
    "from neural_orientation_field.nerf.utils import cam_ray_from_pose, nerf_image_render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72cdc2d6-7b8c-4ea3-8de2-92d1f7e082ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use MPS device.\n",
    "USE_DEVICE = \"mps\"\n",
    "\n",
    "if USE_DEVICE == \"mps\" and torch.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif USE_DEVICE == \"cuda\" and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d94d29f-25af-43ef-a89f-f7201634c764",
   "metadata": {},
   "source": [
    "## Load Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1309aa43-ee33-40b1-9474-e089c7a1308d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/Users/fangjun/Documents/stanford/cs229/final-project/data/images/blender-hair-long-test/rendered'),\n",
       " PosixPath('/Users/fangjun/Documents/stanford/cs229/final-project/data/camera/blender-hair-long-test/frame-names.txt'),\n",
       " PosixPath('/Users/fangjun/Documents/stanford/cs229/final-project/data/camera/blender-hair-long-test/camera-transforms.npy'),\n",
       " PosixPath('/Users/fangjun/Documents/stanford/cs229/final-project/data/camera/blender-hair-long-test/camera-params.npy'),\n",
       " PosixPath('/Users/fangjun/Documents/stanford/cs229/final-project/data/output/nerf/blender-hair-long'),\n",
       " PosixPath('/Users/fangjun/Documents/stanford/cs229/final-project/data/eval/nerf/blender-hair-long'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input\n",
    "IMAGE_PATH = \"../../data/images/blender-hair-long-test/rendered/\"\n",
    "CAMERA_PATH = \"../../data/camera/blender-hair-long-test/\"\n",
    "CHECKPOINT_PATH = \"../../data/output/nerf/blender-hair-long/\"\n",
    "OUTPUT_PATH = \"../../data/eval/nerf/blender-hair-long\"\n",
    "\n",
    "image_path = pathlib.Path(IMAGE_PATH).resolve()\n",
    "if not image_path.exists():\n",
    "    raise FileNotFoundError(\"Image path doesn't exist.\")\n",
    "camera_path = pathlib.Path(CAMERA_PATH).resolve()\n",
    "if not camera_path.exists():\n",
    "    raise FileNotFoundError(\"Camera path doesn't exist.\")\n",
    "checkpoint_path = pathlib.Path(CHECKPOINT_PATH).resolve()\n",
    "if not checkpoint_path.exists():\n",
    "    raise FileNotFoundError(\"Checkpoint path doesn't exist.\")\n",
    "output_path = pathlib.Path(OUTPUT_PATH).resolve()\n",
    "if not output_path.exists():\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "frame_name_path = camera_path / \"frame-names.txt\"\n",
    "cam_transform_path = camera_path / \"camera-transforms.npy\"\n",
    "cam_param_path = camera_path / \"camera-params.npy\"\n",
    "\n",
    "image_path, frame_name_path, cam_transform_path, cam_param_path, checkpoint_path, output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c475ba8-ffb1-4720-8106-b15fa4eaff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset.\n",
    "with open(frame_name_path, \"r\") as frame_path_file:\n",
    "    frame_names = frame_path_file.read().split(\"\\n\")\n",
    "    frame_paths = [image_path / frame_name for frame_name in frame_names]\n",
    "with open(cam_transform_path, \"rb\") as cam_transform_file:\n",
    "    cam_transforms = np.load(cam_transform_file)\n",
    "with open(cam_param_path, \"rb\") as cam_param_file:\n",
    "    cam_params = np.load(cam_param_file)\n",
    "image_dataset = NeRFPriorImageDataset(frame_paths, cam_params, cam_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b224b28-f741-4864-9ee0-fd05d378762e",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6251b105-516f-4b85-9e87-8cf8d2eb1f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params_file_name = \"model_params.pth\"\n",
    "coarse_model_file_name = \"coarse_final.pth\"\n",
    "fine_model_file_name = \"fine_final.pth\"\n",
    "\n",
    "# Load model params.\n",
    "model_params = torch.load(checkpoint_path / model_params_file_name, weights_only=True)\n",
    "\n",
    "coarse_pos_encode = model_params[\"coarse_pos_encode\"]\n",
    "fine_pos_encode = model_params[\"fine_pos_encode\"]\n",
    "nc = model_params[\"nc\"]\n",
    "fc = model_params[\"fc\"]\n",
    "samples_per_ray = model_params[\"samples_per_ray\"]\n",
    "max_subd_samples = model_params[\"max_subd_samples\"]\n",
    "\n",
    "# Load model.\n",
    "coarse_model = NeRfCoarseModel(num_encoding_functions=coarse_pos_encode)\n",
    "coarse_model.to(device)\n",
    "fine_model = NeRfFineModel(num_encoding_functions=fine_pos_encode)\n",
    "fine_model.to(device)\n",
    "\n",
    "coarse_model.load_state_dict(torch.load(checkpoint_path / coarse_model_file_name, weights_only=True))\n",
    "fine_model.load_state_dict(torch.load(checkpoint_path / fine_model_file_name, weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9f9d4b-189a-4a98-9c0a-517dd80c3b0b",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "981f498a-653d-45ba-bd0c-242aad3b59fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime parameters.\n",
    "ray_batch_size = 16384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69f007f3-8fb9-4c97-a74c-6bc5773f6032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9c7f0dba3b47248ccc15b0a53b0ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_images = []\n",
    "pred_images = []\n",
    "for test_image, cam_transform, (h, w), (f, cx, cy) in tqdm(image_dataset):\n",
    "    cam_orig, cam_ray_world = cam_ray_from_pose(cam_transform, h, w, f, cx, cy)\n",
    "    _, fine_pred = nerf_image_render(\n",
    "        coarse_model,\n",
    "        fine_model,\n",
    "        cam_orig,\n",
    "        cam_ray_world,\n",
    "        ray_batch_size,\n",
    "        nc,\n",
    "        fc,\n",
    "        samples_per_ray,\n",
    "        max_subd_samples,\n",
    "        coarse_pos_encode,\n",
    "        fine_pos_encode,\n",
    "        device\n",
    "    )\n",
    "    test_images.append(test_image)\n",
    "    pred_images.append(fine_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29b3d4ee-a78d-4c13-b753-64eb70c7e02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d467a7b0bdf0460ea84b8e40b119c93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(range(len(test_images))):\n",
    "    test_image = Image.fromarray((test_images[i] * 255).astype(np.uint8))\n",
    "    test_image.save(output_path / f\"test_{i}.png\")\n",
    "    pred_image = Image.fromarray((pred_images[i].detach().numpy() * 255).astype(np.uint8))\n",
    "    pred_image.save(output_path / f\"pred_{i}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
