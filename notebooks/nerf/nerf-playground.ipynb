{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e115a31-a121-4d2e-82e3-5a11eb762be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "import pycolmap\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import RandomSampler, DataLoader, random_split\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import pyvista as pv\n",
    "\n",
    "import neural_orientation_field.utils as utils\n",
    "import neural_orientation_field.colmap.colmap_utils as colutils\n",
    "from neural_orientation_field.nerf.dataset import NeRFImageDataset, NeRFRayDataset\n",
    "from neural_orientation_field.nerf.model import NeRfCoarseModel, NeRfFineModel\n",
    "from neural_orientation_field.nerf.utils import pos_encode, static_volumetric_renderer, adaptive_volumetric_renderer, cam_ray_from_pose, nerf_image_render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354ec5df-70a4-4e13-81e7-fd73c33bfb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed rng for reproducability\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dab1fca-d995-408d-9c66-792cf1e90216",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c692271-b08d-45c4-81f9-d6fdf54f9c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "IMAGE_PATH = \"../../data/images/blender-hair-long/rendered/\"\n",
    "CAMERA_PATH = \"../../data/cameras/blender-hair-long/\"\n",
    "CHECKPOINT_PATH = \"../../data/models/nerf/blender-hair-long/\"\n",
    "\n",
    "image_path = pathlib.Path(IMAGE_PATH).resolve()\n",
    "camera_path = pathlib.Path(CAMERA_PATH).resolve()\n",
    "checkpoint_path = pathlib.Path(CHECKPOINT_PATH).resolve()\n",
    "if not checkpoint_path.exists():\n",
    "    checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "frame_name_path = camera_path / \"frame-names.txt\"\n",
    "cam_transform_path = camera_path / \"camera-transforms.npy\"\n",
    "cam_param_path = camera_path / \"camera-params.npy\"\n",
    "\n",
    "image_path, frame_name_path, cam_transform_path, cam_param_path, checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e86b953-8e60-4858-a4cc-3cb6445032a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blender Image Dataset\n",
    "with open(frame_name_path, \"r\") as frame_path_file:\n",
    "    frame_names = frame_path_file.read().split(\"\\n\")\n",
    "    frame_paths = [image_path / frame_name for frame_name in frame_names]\n",
    "with open(cam_transform_path, \"rb\") as cam_transform_file:\n",
    "    cam_transforms = np.load(cam_transform_file)\n",
    "with open(cam_param_path, \"rb\") as cam_param_file:\n",
    "    cam_params = np.load(cam_param_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66e4f33-e6e3-4a51-870e-ca03c35bce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set size\n",
    "num_valid = 2\n",
    "\n",
    "image_dataset = NeRFImageDataset(frame_paths, cam_params, cam_transforms)\n",
    "num_train = len(image_dataset) - num_valid\n",
    "image_dataset_train, image_dataset_valid = random_split(image_dataset, [num_train, num_valid])\n",
    "len(image_dataset_train), len(image_dataset_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e67b898-5096-4196-a04c-cc4d3177c0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=len(image_dataset_train), desc=\"Processing Image\") as progress:\n",
    "    ray_dataset = NeRFRayDataset(image_dataset_train, progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c341451a-91d6-45a5-85bc-a07dd4823e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "image, cam_transform, (h, w), (f, cx, cy) = image_dataset[idx]\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc23bb-8e0f-4447-9ac8-ecf7dcd4d681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize camera rays.\n",
    "# origins = np.tile(cam_orig[np.newaxis, np.newaxis, :], (image.shape[0], image.shape[1], 1)).reshape(-1, 3)\n",
    "# directions = cam_ray_world.reshape(-1, 3)\n",
    "# colors = image.reshape(-1, 3)\n",
    "# sample_idxs = np.arange(0, colors.shape[0])\n",
    "# np.random.shuffle(sample_idxs)\n",
    "# sample_idxs = sample_idxs[:1024]\n",
    "# origins = origins[sample_idxs]\n",
    "# directions = directions[sample_idxs]\n",
    "# colors = colors[sample_idxs]\n",
    "\n",
    "# origins = []\n",
    "# directions = []\n",
    "# colors = []\n",
    "# for i in range(65535):\n",
    "#     idx = random.randint(0, len(ray_dataset))\n",
    "#     origin, direction, color = ray_dataset[idx]\n",
    "#     origins.append(origin)\n",
    "#     directions.append(direction)\n",
    "#     colors.append(color)\n",
    "# origins = np.array(origins)\n",
    "# directions = np.array(directions)\n",
    "# colors = np.array(colors)\n",
    "# \n",
    "# # Normalize directions if necessary\n",
    "# directions = directions / np.linalg.norm(directions, axis=1)[:, np.newaxis]\n",
    "# \n",
    "# # Create the PyVista PolyData object\n",
    "# vectors = pv.PolyData(origins)\n",
    "# \n",
    "# # Add the directions and colors as point data\n",
    "# vectors[\"directions\"] = directions\n",
    "# vectors[\"colors\"] = colors\n",
    "# \n",
    "# # Create the glyphs (arrows)\n",
    "# arrows = vectors.glyph(orient=\"directions\", scale=False, factor=1)\n",
    "# \n",
    "# # Plot the arrows with colors\n",
    "# plotter = pv.Plotter()\n",
    "# plotter.add_mesh(arrows, scalars=\"colors\", rgb=True)\n",
    "# plotter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d082a6-e6dd-4dcf-a9ee-5566243b03fc",
   "metadata": {},
   "source": [
    "# NeRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2312eea-1e81-4a72-8f65-885683f4e841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MPS device.\n",
    "USE_DEVICE = \"mps\"\n",
    "\n",
    "if USE_DEVICE == \"mps\" and torch.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif USE_DEVICE == \"cuda\" and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f57e199-da2c-45a6-8e65-5c8e649f8801",
   "metadata": {},
   "source": [
    "## NeRF Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa452ae5-89c5-4add-8bd3-c7926e544a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Near/far clipping distance.\n",
    "nc = 1\n",
    "fc = 8\n",
    "# Positional encoding number.\n",
    "coarse_pos_encode = 2\n",
    "fine_pos_encode = 4\n",
    "samples_per_ray = 4\n",
    "max_subd_samples = 4\n",
    "# Hyper parameters.\n",
    "lr = 2e-4\n",
    "num_epochs = 1\n",
    "ray_batch_size = 8192\n",
    "# Training settings.\n",
    "size_train_ray = 0.25\n",
    "valid_per_epoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e096574-5b04-443d-a899-ae132e2dbe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init model.\n",
    "coarse_model = NeRfCoarseModel(num_encoding_functions=coarse_pos_encode)\n",
    "coarse_model.to(device)\n",
    "coarse_optimizer = torch.optim.Adam(coarse_model.parameters(), lr=lr)\n",
    "\n",
    "fine_model = NeRfFineModel(num_encoding_functions=fine_pos_encode)\n",
    "fine_model.to(device)\n",
    "fine_optimizer = torch.optim.Adam(fine_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c544ea76-1672-4707-9a0d-a2c0e1313b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_model.train()\n",
    "fine_model.train()\n",
    "\n",
    "train_sampler = RandomSampler(data_source=ray_dataset, num_samples=int(size_train_ray * len(ray_dataset)))\n",
    "dataloader = DataLoader(\n",
    "    ray_dataset,\n",
    "    sampler=train_sampler,\n",
    "    batch_size=ray_batch_size,\n",
    ")\n",
    "\n",
    "writer = SummaryWriter(flush_secs=1)\n",
    "valid_images = np.array(\n",
    "    [valid_image for valid_image, _, _, _ in image_dataset_valid]\n",
    ")\n",
    "writer.add_image(\"Valid Image Ground Truth\",\n",
    "                 valid_images, dataformats=\"NHWC\")\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "best_model_coarse = None\n",
    "best_model_fine = None\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    valid_every_n_batch = int(len(dataloader) / valid_per_epoch)\n",
    "    # One iteration of the training.\n",
    "    for batch_i, (cam_orig_batch, cam_ray_batch, color_batch) in enumerate(tqdm(dataloader)):\n",
    "        cam_orig_batch = cam_orig_batch.type(torch.float32).to(device)\n",
    "        cam_ray_batch = cam_ray_batch.type(torch.float32).to(device)\n",
    "        color_batch = color_batch.type(torch.float32).to(device)\n",
    "        coarse_color_pred, occupancy, sample_depth = static_volumetric_renderer(\n",
    "            coarse_model,\n",
    "            cam_orig_batch.reshape(-1, 3),\n",
    "            cam_ray_batch.reshape(-1, 3),\n",
    "            nc,\n",
    "            fc,\n",
    "            num_sample=samples_per_ray,\n",
    "            num_pos_encode=coarse_pos_encode,\n",
    "            device=device\n",
    "        )\n",
    "        coarse_loss = torch.nn.functional.mse_loss(coarse_color_pred, color_batch)\n",
    "        fine_color_pred, _, _ = adaptive_volumetric_renderer(\n",
    "            fine_model,\n",
    "            cam_orig_batch.reshape(-1, 3),\n",
    "            cam_ray_batch.reshape(-1, 3),\n",
    "            occupancy,\n",
    "            sample_depth,\n",
    "            max_subd_sample=max_subd_samples,\n",
    "            num_pos_encode=fine_pos_encode,\n",
    "            device=device\n",
    "        )\n",
    "        fine_loss = torch.nn.functional.mse_loss(fine_color_pred, color_batch)\n",
    "        loss = coarse_loss + fine_loss\n",
    "        loss.backward()\n",
    "        coarse_optimizer.step()\n",
    "        coarse_optimizer.zero_grad()\n",
    "        fine_optimizer.step()\n",
    "        fine_optimizer.zero_grad()\n",
    "        writer.add_scalar(\"Coarse Loss Train\", coarse_loss,\n",
    "                          (epoch * len(dataloader) + batch_i) * ray_batch_size)\n",
    "        writer.add_scalar(\"Fine Loss Train\", fine_loss,\n",
    "                          (epoch * len(dataloader) + batch_i) * ray_batch_size)\n",
    "\n",
    "        if batch_i % valid_every_n_batch == 0:\n",
    "            coarse_model.eval()\n",
    "            fine_model.eval()\n",
    "\n",
    "            coarse_preds = []\n",
    "            fine_preds = []\n",
    "            coarse_losses = []\n",
    "            fine_losses = []\n",
    "            for valid_image, cam_transform, (h, w), (f, cx, cy) in image_dataset_valid:\n",
    "                cam_orig, cam_ray_world = cam_ray_from_pose(\n",
    "                    cam_transform, h, w, f, cx, cy)\n",
    "                coarse_pred, fine_pred = nerf_image_render(\n",
    "                    coarse_model,\n",
    "                    fine_model,\n",
    "                    cam_orig,\n",
    "                    cam_ray_world,\n",
    "                    ray_batch_size,\n",
    "                    nc,\n",
    "                    fc,\n",
    "                    samples_per_ray,\n",
    "                    max_subd_samples,\n",
    "                    coarse_pos_encode,\n",
    "                    fine_pos_encode,\n",
    "                    device\n",
    "                )\n",
    "                coarse_preds.append(coarse_pred)\n",
    "                fine_preds.append(fine_pred)\n",
    "                valid_image = torch.tensor(valid_image)\n",
    "                coarse_loss = torch.nn.functional.mse_loss(\n",
    "                    coarse_pred, valid_image)\n",
    "                fine_loss = torch.nn.functional.mse_loss(\n",
    "                    fine_pred, valid_image)\n",
    "                coarse_losses.append(coarse_loss)\n",
    "                fine_losses.append(fine_loss)\n",
    "\n",
    "            coarse_preds = np.array(coarse_preds)\n",
    "            fine_preds = np.array(fine_preds)\n",
    "            coarse_loss_valid = np.array(coarse_losses).mean()\n",
    "            fine_loss_valid = np.array(fine_losses).mean()\n",
    "            loss_valid = coarse_loss_valid + fine_loss_valid\n",
    "            if loss_valid < best_loss:\n",
    "                best_model_coarse = coarse_model.state_dict()\n",
    "                best_model_fine = fine_model.state_dict()\n",
    "                best_loss = loss_valid\n",
    "            writer.add_scalar(\"Coarse Loss Valid\", coarse_loss_valid,\n",
    "                              (epoch * len(dataloader) + batch_i) * ray_batch_size)\n",
    "            writer.add_scalar(\"Fine Loss Valid\", fine_loss_valid,\n",
    "                              (epoch * len(dataloader) + batch_i) * ray_batch_size)\n",
    "            writer.add_image(\"Rendered Validation Image Coarse\", coarse_preds, (epoch *\n",
    "                             len(dataloader) + batch_i) * ray_batch_size, dataformats=\"NHWC\")\n",
    "            writer.add_image(\"Rendered Validation Image Fine\", fine_preds, (epoch *\n",
    "                             len(dataloader) + batch_i) * ray_batch_size, dataformats=\"NHWC\")\n",
    "            coarse_model.train()\n",
    "            fine_model.train()\n",
    "\n",
    "    torch.save(coarse_model.state_dict(), checkpoint_path / f\"coarse_epoch_{epoch}.pth\")\n",
    "    torch.save(fine_model.state_dict(), checkpoint_path / f\"fine_epoch_{epoch}.pth\")\n",
    "writer.close()\n",
    "\n",
    "if best_model_coarse:\n",
    "    coarse_model.load_state_dict(best_model_coarse)\n",
    "if best_model_fine:\n",
    "    fine_model.load_state_dict(best_model_fine)\n",
    "\n",
    "torch.save(coarse_model.state_dict(), checkpoint_path / f\"coarse_final.pth\")\n",
    "torch.save(fine_model.state_dict(), checkpoint_path / f\"fine_final.pth\")\n",
    "\n",
    "model_params = {\n",
    "    \"coarse_pos_encode\": coarse_pos_encode,\n",
    "    \"fine_pos_encode\": fine_pos_encode,\n",
    "    \"nc\": nc,\n",
    "    \"fc\": fc,\n",
    "    \"samples_per_ray\": samples_per_ray,\n",
    "    \"max_subd_samples\": max_subd_samples,\n",
    "}\n",
    "torch.save(model_params, checkpoint_path / f\"model_params.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
