\documentclass{article}

\usepackage[accepted]{icml2024}

\usepackage{datetime}
\usepackage{amsfonts}
\usepackage{amsmath}

\usepackage{graphicx}
\usepackage{subcaption}

\usepackage{hyperref}

\begin{document}

\twocolumn[
\icmltitle{Learning Neural Orientation Field for Volumetric Hair Reconstruction}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Fangjun Zhou}{}
\icmlauthor{Weiran Xu}{}
\icmlauthor{Zhenyu Zhang}{}
\end{icmlauthorlist}

\icmlcorrespondingauthor{Fangjun Zhou}{fzhou48@stanford.edu}
\icmlcorrespondingauthor{Weiran Xu}{weiran@stanford.edu}
\icmlcorrespondingauthor{Zhenyu Zhang}{zhenyuz5@stanford.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML, NeRF, MVS}

\vskip 0.3in
]

\section{Introduction}

% Project motivation

Reconstructing human hair is one of the most challenging yet critical process in rendering photorealistic digital human. Unlike other parts of the human body, human hair is highly detailed and often intertwined together. Therefore, it's difficult to use traditional photogrammetry method to reconstruct its structure.

Before machine learning model is used in this field, artists often hand crafted splines on skulls to represent hair strands. Each strand is then textured and rendered to mimic the hair volume. This workflow requires a lot of experience as it's non-trivial for artists to infer the final render result from hair stand splines. To reduce the workload and improve the accuracy of hair reconstruction, machine learning models are trained to generate hair strand from captured images.

In this work, we propose a new method capturing the hair structure by learning a 3D orientation field from $\mathbb{R}^{3} \rightarrow \mathbb{R}^{2}$ representing the hair growing direction. On top of that, another occupancy field from $\mathbb{R}^{3} \rightarrow \mathbb{R}^{2}$ is learned to indicate hair and body occupancy. These mappings can be used later to generate hair strand directly by numerically integrating the orientation field. It can also be used as a latent variable to guide other generative models as mentioned in \cite{metzer_latent-nerf_2022}.

%% TODO: Include the input and output of the model. DONE

Our algorithm fits the 3D orientation field by a simple MLP model. The input of this model is a sample position in space, and the model predicts the hair orientation and occupancy at the sampled position. We also proposed a volumetric renderer for orientation fields capable of rendering the 2D hair orientation map for unseen views. Since this volumetric rendering algorithm is differentiable, we are able to train the model on multiple 2D hair orientation map from different viewing angles.

\section{Related Work}

Previous attempts to 3D hair reconstruction mainly focused on learning-based hair strand generation. This includes some studies about single view hair synthesis \cite{saito_3d_2018, zheng_hairstep_2023, wu_neuralhdhair_2022, ma_single-view_nodate}. Since the image only contains hair structure from one viewing angle, it's impossible to reconstruct entire hair accurately. These models often use pretrained image encoders such as ResNet-50 \cite{saito_3d_2018} to encode the abstract hair style into a feature vector, then use generative models such as U-Net \cite{zheng_hairstep_2023}, VAE \cite{saito_3d_2018}, and diffusion models \cite{sklyarova_neural_2023} to generate the final strand. These models also struggle with generating curly hair as there's only limited information about growing direction after feature extraction.

In \cite{sklyarova_neural_2023} and \cite{rosu_neural_2022}, the authors also tried hair syntheses from multi-view images. However, these two studies still failed to capture finer details.

Another study about this topic tried expanding the traditional PatchMatch MVS (PMVS) algorithm to a Line-based PatchMatch MVS (LPMVS) \cite{nam_strand-accurate_nodate}. This method, despite its high accuracy, doesn't capture the volumetric property of human hair.

Our work is highly inspired by NeRF \cite{mildenhall_nerf_2020}, a model used for 3D reconstruction from 2D images. However, our model differs in two major way.

In NeRF, the model fits a radiance field from $\mathbb{R}^{5} \rightarrow \mathbb{R}^{4}$. The input of the radiance function includes the sample position and camera ray direction. However, since the orientation field our model fits is view-independent, the input space is only $\mathbb{R}^{3}$. This makes it easier for the model to capture more information from a smaller dataset.

On top of that, the volumetric renderer on orientation field also differs from the one on radiance field. When sampling the radiance field, NeRF only integrates the sampled color for each ray, while our model needs to project the sample orientation onto the filming plane and then integrate the projected orientation.


\section{Dataset}

%% TODO: Data generation. (Fangjun) DONE

The dataset used by this project is generated by a Blender geometry node-based hair system demo file \cite{foundation_blender_nodate}. We render the model from multiple viewing angles and exported the camera intrinsics and extrinsics for later use.

Each instance includes a ray-traced image rendered by Cycles renderer, a body mask, a hair mask, and a screen space hair orienrtation map (Figure~\ref{fig:dataset}). The screen space hair orienrtation map stores the projected hair orientation in red and green channel. To render this map, we calculated the world space hair orientation with a custom geometry node and rendered the screen space orientation with a custom shader and AOV render pass.

Our training set consists of 128 instances and our test set consists of 16 instances, all rendered from the same model but with different camera poses.

\begin{figure}[h]
	\centering
	\begin{subfigure}{0.11\textwidth}
		\centering
		\includegraphics[width=\textwidth]{./images/dataset/0009_rendered.png}
		\caption{rendered}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.11\textwidth}
		\centering
		\includegraphics[width=\textwidth]{./images/dataset/0009_bodymask.png}
		\caption{body mask}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.11\textwidth}
		\centering
		\includegraphics[width=\textwidth]{./images/dataset/0009_hairmask.png}
		\caption{hair mask}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.11\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/dataset/0009_hairdir.png}
        \caption{orientation}
	\end{subfigure}

	\caption{Synthetic dataset rendered by Blender}
	\label{fig:dataset}
\end{figure}

\section{Method}

% TODO: Model structure. (Fangjun) DONE
% TODO: Volumetric renderer for vector field. (Fangjun)
% TODO: Occupancy integration and mask loss. (Fangjun)
% TODO: Hierachical volumetric sampling for vector field. (Fangjun)

\subsection{Neural Orientation Field (NeOF) Model}

The neural orientation field (NeOF) model we proposed consists of multiple MLP layers and residual connections. The input of the network is the spatial coordinates $(x, y, z)$ of the point to be sampled. The output is the world space hair orientation $(\theta, \phi)$, as well as the opacity parameter $\sigma = (\sigma_{hair}, \sigma_{body})$.

\subsection{Volumetric Rendering for Neural Orientation Field}

As the training data only consists of screen space hair orientation. We proposed a differentiable volumetric renderer $VolumetricRenderer(O_{world}, \Sigma)$ to render screen space hair orientation as well as body/hair masks from sampled world space orientation $O_{world} = \{ \mathbf{o}_{world}^{(i)} \}$ and occupancy $\Sigma = \{ \sigma^{(i)} \}$. To sample $O_{world}$ and $\Sigma$ for a pixel, we emit a camera ray with the known camera intrinsics and extrinsics on the filming plane. Then, the world space hair orientation and occupancy can be sampled along the camera ray. The volumetric rendering pipeline first projects the world space orientation to screen space, then integrate the screen space orientation with the sampled occupancy.

To project hair orientation, we first need to convert the world space orientation $\mathbf{o}_{world}$ to an orientation vector in homogeneous coordinate $\mathbf{v}_{world} \in \mathbb{R}^{4}$. Given the camera extrinsic matrix $M \in \mathbb{R}_{4x4}$, $\mathbf{v}_{world}$ can be projected to the view space by $\mathbf{v}_{view} = M\mathbf{v}_{world}$.

Normally, to project a world space vector onto screen space, we need to first apply view transform to get view space projection, then apply perspective transform to get screen space projection. However, as we choose pinhole camera as our camera model, the screen space orientation vectors are independent of the camera intrinsics. In other words, $\mathbf{v}_{screen} = \mathbf{v}_{view} = M\mathbf{v}_{world}$

Similar to the volumetric rendering function defined in NeRF \cite{mildenhall_nerf_2020}, our rendering function for screen space orientation can be written as

\begin{align}
	V(\mathbf{r}) & = \int_{t_{n}}^{t_{f}} T(t) \sigma_{hair}(\mathbf{r}(t)) M \mathbf{v}_{world}(\mathbf{r}(t)) dt \\
	T(t) & = exp(-\int_{t_{n}}^{t} \sigma_{body}(\mathbf{r}(s)) ds)
\end{align}

One important difference between our model and NeRF is we use two separate occupancy for body and hair, while in NeRF only one occupancy is used to integrate camera rays. In our case, camera rays can be blocked by face and body that doesn't contribute to the final integration. Therefore, we use $\sigma_{body}$ instead of $\sigma_{hair}$ to integrate residual ray $T(t)$.

As we provide body mask and hair mask in the training data, we can also use the following rendering function to render body mask $B(\mathbf{r})$ and hair mask $H(\mathbf{r})$:

\begin{align}
	B(\mathbf{r}) & = 1 - exp(-\int_{t_{n}}^{t_{f}} \sigma_{body}(\mathbf{r}(s)) ds) \\
	H(\mathbf{r}) & = 1 - exp(-\int_{t_{n}}^{t_{f}} \sigma_{hair}(\mathbf{r}(s)) ds)
\end{align}

Similar to the numerical estimation of radiance integration mentioned in NeRF, we use numerical estimation of orientation and mask integration defined as follows:

\begin{align}
	\hat{V}(\mathbf{r}) & = \sum_{i=1}^{N} T_{i} (1 - exp(-\sigma_{hair}^{(i)} \delta_{i})) M \mathbf{v}_{world}^{(i)} \\
	T_{i} & = exp(-\sum_{j=1}^{i} \sigma_{body}^{(j)} \delta_{j}) \\
	\hat{B}(\mathbf{r}) & = 1 - exp(-\sum_{i=1}^{N} \sigma_{body}^{(i)} \delta_{i}) \\
	\hat{H}(\mathbf{r}) & = 1 - exp(-\sum_{i=1}^{N} \sigma_{hair}^{(i)} \delta_{i})
\end{align}

where $\delta_{i} = t_{i + 1} - t_{i}$. $\hat{V}(\mathbf{r})$, $\hat{B}(\mathbf{r})$, and $\hat{H}(\mathbf{r})$ are the final outputs of our volumetric render. The final loss is defined as the sum of orientation loss and mask losses. In theory, only orientation loss is required for the model to converge. However, we observed that introducing the mask losses helps improving the convergence speed. It also improves the model's performance on occupancy prediction accuracy.

\section{Experiment}

% TODO: Experiement setup. (Fangjun, Zhenyu) DONE
% TODO: Implementation Details. (Fangjun)
% TODO: Baseline setup. DONE

\subsection{Implementation Details}

\subsubsection{Position Encoding and Hierarchical Volume Sampling}

We use position encoding and hierarchical volume sampling mentioned in \cite{mildenhall_nerf_2020} to improve convergence rate in our implementation.

The position encoding function is defined as:

\begin{align}
    \gamma(p)_{2i} &= sin(2^{i} \pi p) \\
    \gamma(p)_{2i + 1} &= cos(2^{i} \pi p)
\end{align}

As mentioned in \cite{mildenhall_nerf_2020}, deep neural networks tend to fit low frequency functions. Using position encoding functions helps improve the convergence rate on high frequency data. This is extremely important for our application as the spatial frequency of hair orientation and occupancy is very high. In our experiment, at least 8 levels of position encoding is required to capture detailed hair directions.

For hierarchical volume sampling, we trained a coarse and a fine model simultaneously to fit the scene. The coarse model is used with an even sample depth to render the orientation map. The sampled body occupancy is then used to generate subdivided sample depth for the fine model. Areas with higher body occupancy in the coarse model will receive higher sample rate when rendered with the fine model. As mentioned previously, due to the high frequency nature of hair structure, hierarchical volume sampling is also always required in practice.

\subsubsection{NeOF Model Training and Test Setup}

Our model is trained on a training set of 128 2D orientation maps of size 512x512. We use Adam optimizer with $2e-4$ learning rate. For the coarse model, we use 6 levels of position encoding with a fixed sample rate of 16. For the fine model, we use 8 levels of position encoding with the maximum subdivided sample rate of 8. The model is trained for 8 epochs and the checkpoint with lowest loss on validation set is save.

% Duplicated dataset section, removed.
% \subsection{Datasets} 

% To perform more precise model evaluation, we generated a dataset using Blender. For the same individual, the dataset includes 144 RGB hair photos with a resolution of 512x512 taken from different viewpoints, corresponding body and hair masks, camera parameters and pose matrices, as well as 2D hair orientations observed from the camera positions. The dataset for each individual is split into a training set with 128 images, a validation set with 16 images, and a test set with 16 images. 

\subsection{Metrics}

To evaluate the performance of the model, we rendered the orientation field on 16 unseen camera views in the test set and evaluate the performance using rendered 2D orientation.

We use PSNR (Peak Signal-to-Noise Ratio) for evaluation. The reconstructed 2D hair orientation map is compared with the ground truth 2D hair orientation by calculating the MSE, and the MSEs from all different viewpoints are averaged. This average MSE is then used to compute the overall PSNR metric. The complete formula is as follows:

\begin{align}
    \text{MSE}_k &= \frac{1}{M \cdot N \cdot 2} \\
    & \quad \sum_{i=1}^M \sum_{j=1}^N \sum_{c=1}^2 \left(I_k (i,j,c) - K_k(i,j,c) \right)^2 \\
    \text{MSE}_\text{avg} &= \frac{1}{N_\text{images}} \sum_{k=1}^{N_\text{images}}\text{MSE}_k \\
    \text{PSNR}_\text{avg} &= 10 \cdot \log_{10} \left( \frac{\text{MAX}^2}{\text{MSE}_{\text{avg}}} \right)
\end{align}

where $\text{MSE}_k$ is the MSE of image $k$ and its ground truth, $I_k$ and $K_k$ are the reconstructed 2D hair orientation for image $k$, $N$ and $M$ is the height and weight of the image, $N_\text{images}$ is the number of images, $\text{MSE}_\text{avg}$ is the average MSE of these image, MAX is the maximum possible value of each pixel (here we use 8-bit image, so it is 255), and $\text{PSNR}_\text{avg}$ is our final PSNR value.

\subsection{Baseline Method}

We trained a NeRF model to represent the hair structure as a radiance field, leveraging it to infer hair orientation. Since NeRF does not inherently encode orientation information, we relied on HairStep to predict orientation. Observed that the output of NeRF is often blurred, consistent with findings from the original NeRF paper \cite{mildenhall_nerf_2020}, which noted that NeRF tends to capture low-frequency features early in training, making HairStep difficult to segment out hair and output reasonable hair direction, thus unable to maintain relevant direction information; see Figure \ref{fig:nerf_hairstep}.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \begin{subfigure}{0.48\textwidth}
            \centering
            \includegraphics[width=\textwidth]{./images/baseline-method/test_6_rendered.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.48\textwidth}
            \centering
            \includegraphics[width=\textwidth]{./images/baseline-method/test_6_hairstep.jpg}
        \end{subfigure}
        \caption{origin rendered with hairstep}
    \end{subfigure}
    \\
    \begin{subfigure}{0.48\textwidth}
        \begin{subfigure}{0.48\textwidth}
            \centering
            \includegraphics[width=\textwidth]{./images/baseline-method/pred_6_nerf.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.48\textwidth}
            \centering
            \includegraphics[width=\textwidth]{./images/baseline-method/pred_6_hairstep.jpg}
        \end{subfigure}
        \caption{nerf predicted with hairstep}
    \end{subfigure}

    \caption{Hairstep on rendered image/NeRF prediction}
    \label{fig:nerf_hairstep}
\end{figure}

\section{Result}

% TODO: Result. (Weiran, Zhenyu)

\section{Conclusion}

% TODO: Conclusion. (Weiran, Zhenyu)

\newpage

\bibliographystyle{plainnat}
\bibliography{references}

\newpage

\section{Contribution}

\subsection{Fangjun Zhou}

COLMAP feature and camera pose preprocessing script; \texttt{colmap\_visualizer} with imagui and pyvista to help visualize the extracted point cloud/camera position; Blender camera pose extraction script and dataset generation; NeRF and NeOF camera ray generation algorithm, volumetric renderer, hierachical volume sampling, training script implementation; Introduction, related work, dataset, method sections of final report.

\subsection{Weiran Xu}

Contributed to building the human portrait dataset and applied HairStep for image preprocessing and hair strand direction extraction, preparing the data for analysis. Also wrote part of experiment section in the milestone report, detailing methodology and preliminary findings.

\subsection{Zhenyu Zhang}

Responsible for refactoring Tiny NeRF, do the training and testing related to Tiny NeRF, and implementing the code for 3D vector field reconstruction of hair growth. This included model code, loss computation code, camera view transformation and coordinate system conversion code, as well as the code of calculations for remapping 3D vectors in space to the camera frame.

\end{document}
