
@misc{zheng_hairstep_2023,
	title = {{HairStep}: Transfer Synthetic to Real Using Strand and Depth Maps for Single-View 3D Hair Modeling},
	url = {http://arxiv.org/abs/2303.02700},
	doi = {10.48550/arXiv.2303.02700},
	shorttitle = {{HairStep}},
	abstract = {In this work, we tackle the challenging problem of learning-based single-view 3D hair modeling. Due to the great difficulty of collecting paired real image and 3D hair data, using synthetic data to provide prior knowledge for real domain becomes a leading solution. This unfortunately introduces the challenge of domain gap. Due to the inherent difficulty of realistic hair rendering, existing methods typically use orientation maps instead of hair images as input to bridge the gap. We firmly think an intermediate representation is essential, but we argue that orientation map using the dominant filtering-based methods is sensitive to uncertain noise and far from a competent representation. Thus, we first raise this issue up and propose a novel intermediate representation, termed as {HairStep}, which consists of a strand map and a depth map. It is found that {HairStep} not only provides sufficient information for accurate 3D hair modeling, but also is feasible to be inferred from real images. Specifically, we collect a dataset of 1,250 portrait images with two types of annotations. A learning framework is further designed to transfer real images to the strand map and depth map. It is noted that, an extra bonus of our new dataset is the first quantitative metric for 3D hair modeling. Our experiments show that {HairStep} narrows the domain gap between synthetic and real and achieves state-of-the-art performance on single-view 3D hair reconstruction.},
	number = {{arXiv}:2303.02700},
	publisher = {{arXiv}},
	author = {Zheng, Yujian and Jin, Zirong and Li, Moran and Huang, Haibin and Ma, Chongyang and Cui, Shuguang and Han, Xiaoguang},
	urldate = {2024-10-06},
	date = {2023-03-23},
	eprinttype = {arxiv},
	eprint = {2303.02700 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/fangjun/Zotero/storage/QGMGHNKZ/Zheng et al. - 2023 - HairStep Transfer Synthetic to Real Using Strand .pdf:application/pdf;arXiv.org Snapshot:/Users/fangjun/Zotero/storage/HTQFEICE/2303.html:text/html},
}

@misc{sklyarova_neural_2023,
	title = {Neural Haircut: Prior-Guided Strand-Based Hair Reconstruction},
	url = {http://arxiv.org/abs/2306.05872},
	doi = {10.48550/arXiv.2306.05872},
	shorttitle = {Neural Haircut},
	abstract = {Generating realistic human 3D reconstructions using image or video data is essential for various communication and entertainment applications. While existing methods achieved impressive results for body and facial regions, realistic hair modeling still remains challenging due to its high mechanical complexity. This work proposes an approach capable of accurate hair geometry reconstruction at a strand level from a monocular video or multi-view images captured in uncontrolled lighting conditions. Our method has two stages, with the first stage performing joint reconstruction of coarse hair and bust shapes and hair orientation using implicit volumetric representations. The second stage then estimates a strand-level hair reconstruction by reconciling in a single optimization process the coarse volumetric constraints with hair strand and hairstyle priors learned from the synthetic data. To further increase the reconstruction fidelity, we incorporate image-based losses into the fitting process using a new differentiable renderer. The combined system, named Neural Haircut, achieves high realism and personalization of the reconstructed hairstyles.},
	number = {{arXiv}:2306.05872},
	publisher = {{arXiv}},
	author = {Sklyarova, Vanessa and Chelishev, Jenya and Dogaru, Andreea and Medvedev, Igor and Lempitsky, Victor and Zakharov, Egor},
	urldate = {2024-10-06},
	date = {2023-06-12},
	eprinttype = {arxiv},
	eprint = {2306.05872 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	file = {arXiv Fulltext PDF:/Users/fangjun/Zotero/storage/5KZSJMTK/Sklyarova et al. - 2023 - Neural Haircut Prior-Guided Strand-Based Hair Rec.pdf:application/pdf;arXiv.org Snapshot:/Users/fangjun/Zotero/storage/4F38CUKM/2306.html:text/html},
}

@article{saito_3d_2018,
	title = {3D hair synthesis using volumetric variational autoencoders},
	volume = {37},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3272127.3275019},
	doi = {10.1145/3272127.3275019},
	abstract = {Recent advances in single-view 3D hair digitization have made the creation of high-quality {CG} characters scalable and accessible to end-users, enabling new forms of personalized {VR} and gaming experiences. To handle the complexity and variety of hair structures, most cutting-edge techniques rely on the successful retrieval of a particular hair model from a comprehensive hair database. Not only are the aforementioned data-driven methods storage intensive, but they are also prone to failure for highly unconstrained input images, complicated hairstyles, and failed face detection. Instead of using a large collection of 3D hair models directly, we propose to represent the manifold of 3D hairstyles implicitly through a compact latent space of a volumetric variational autoencoder ({VAE}). This deep neural network is trained with volumetric orientation field representations of 3D hair models and can synthesize new hairstyles from a compressed code. To enable end-to-end 3D hair inference, we train an additional embedding network to predict the code in the {VAE} latent space from any input image. Strand-level hairstyles can then be generated from the predicted volumetric representation. Our fully automatic framework does not require any ad-hoc face fitting, intermediate classification and segmentation, or hairstyle database retrieval. Our hair synthesis approach is significantly more robust and can handle a much wider variation of hairstyles than state-of-the-art data-driven hair modeling techniques with challenging inputs, including photos that are low-resolution, overexposured, or contain extreme head poses. The storage requirements are minimal and a 3D hair model can be produced from an image in a second. Our evaluations also show that successful reconstructions are possible from highly stylized cartoon images, non-human subjects, and pictures taken from behind a person. Our approach is particularly well suited for continuous and plausible hair interpolation between very different hairstyles.},
	pages = {1--12},
	number = {6},
	journaltitle = {{ACM} Transactions on Graphics},
	shortjournal = {{ACM} Trans. Graph.},
	author = {Saito, Shunsuke and Hu, Liwen and Ma, Chongyang and Ibayashi, Hikaru and Luo, Linjie and Li, Hao},
	urldate = {2024-10-06},
	date = {2018-12-31},
	langid = {english},
	file = {Saito et al. - 2018 - 3D hair synthesis using volumetric variational aut.pdf:/Users/fangjun/Zotero/storage/W8QESIHG/Saito et al. - 2018 - 3D hair synthesis using volumetric variational aut.pdf:application/pdf},
}

@inproceedings{wu_neuralhdhair_2022,
	location = {New Orleans, {LA}, {USA}},
	title = {{NeuralHDHair}: Automatic High-fidelity Hair Modeling from a Single Image Using Implicit Neural Representations},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9878513/},
	doi = {10.1109/CVPR52688.2022.00158},
	shorttitle = {{NeuralHDHair}},
	abstract = {Undoubtedly, high-ﬁdelity 3D hair plays an indispensable role in digital humans. However, existing monocular hair modeling methods are either tricky to deploy in digital systems (e.g., due to their dependence on complex user interactions or large databases) or can produce only a coarse geometry. In this paper, we introduce {NeuralHDHair}, a ﬂexible, fully automatic system for modeling high-ﬁdelity hair from a single image. The key enablers of our system are two carefully designed neural networks: an {IRHairNet} (Implicit representation for hair using neural network) for inferring high-ﬁdelity 3D hair geometric features (3D orientation ﬁeld and 3D occupancy ﬁeld) hierarchically and a {GrowingNet} (Growing hair strands using neural network) to efﬁciently generate 3D hair strands in parallel. Specifically, we perform a coarse-to-ﬁne manner and propose a novel voxel-aligned implicit function ({VIFu}) to represent the global hair feature, which is further enhanced by the local details extracted from a hair luminance map. To improve the efﬁciency of a traditional hair growth algorithm, we adopt a local neural implicit function to grow strands based on the estimated 3D hair geometric features. Extensive experiments show that our method is capable of constructing a high-ﬁdelity 3D hair model from a single image, both efﬁciently and effectively, and achieves the-state-of-the-art performance.},
	eventtitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {1516--1525},
	booktitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Wu, Keyu and Ye, Yifan and Yang, Lingchen and Fu, Hongbo and Zhou, Kun and Zhengl, Youyi},
	urldate = {2024-10-06},
	date = {2022-06},
	langid = {english},
	file = {Wu et al. - 2022 - NeuralHDHair Automatic High-fidelity Hair Modelin.pdf:/Users/fangjun/Zotero/storage/BXH4YN45/Wu et al. - 2022 - NeuralHDHair Automatic High-fidelity Hair Modelin.pdf:application/pdf},
}

@article{ma_single-view_nodate,
	title = {Single-View Hair Modeling Using A Hairstyle Database},
	abstract = {Human hair presents highly convoluted structures and spans an extraordinarily wide range of hairstyles, which is essential for the digitization of compelling virtual avatars but also one of the most challenging to create. Cutting-edge hair modeling techniques typically rely on expensive capture devices and signiﬁcant manual labor. We introduce a novel data-driven framework that can digitize complete and highly complex 3D hairstyles from a single-view photograph. We ﬁrst construct a large database of manually crafted hair models from several online repositories. Given a reference photo of the target hairstyle and a few user strokes as guidance, we automatically search for multiple best matching examples from the database and combine them consistently into a single hairstyle to form the large-scale structure of the hair model. We then synthesize the ﬁnal hair strands by jointly optimizing for the projected 2D similarity to the reference photo, the physical plausibility of each strand, as well as the local orientation coherency between neighboring strands. We demonstrate the effectiveness and robustness of our method on a variety of hairstyles and challenging images, and compare our system with state-of-the-art hair modeling algorithms.},
	author = {Ma, Chongyang},
	langid = {english},
	file = {Ma - Single-View Hair Modeling Using A Hairstyle Databa.pdf:/Users/fangjun/Zotero/storage/9BXWYQA5/Ma - Single-View Hair Modeling Using A Hairstyle Databa.pdf:application/pdf},
}

@misc{rosu_neural_2022,
	title = {Neural Strands: Learning Hair Geometry and Appearance from Multi-View Images},
	url = {http://arxiv.org/abs/2207.14067},
	shorttitle = {Neural Strands},
	abstract = {We present Neural Strands, a novel learning framework for modeling accurate hair geometry and appearance from multi-view image inputs. The learned hair model can be rendered in real-time from any viewpoint with high-fidelity view-dependent effects. Our model achieves intuitive shape and style control unlike volumetric counterparts. To enable these properties, we propose a novel hair representation based on a neural scalp texture that encodes the geometry and appearance of individual strands at each texel location. Furthermore, we introduce a novel neural rendering framework based on rasterization of the learned hair strands. Our neural rendering is strand-accurate and anti-aliased, making the rendering view-consistent and photorealistic. Combining appearance with a multi-view geometric prior, we enable, for the first time, the joint learning of appearance and explicit hair geometry from a multi-view setup. We demonstrate the efficacy of our approach in terms of fidelity and efficiency for various hairstyles.},
	number = {{arXiv}:2207.14067},
	publisher = {{arXiv}},
	author = {Rosu, Radu Alexandru and Saito, Shunsuke and Wang, Ziyan and Wu, Chenglei and Behnke, Sven and Nam, Giljoo},
	urldate = {2024-10-06},
	date = {2022-07-28},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2207.14067 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	file = {Rosu et al. - 2022 - Neural Strands Learning Hair Geometry and Appeara.pdf:/Users/fangjun/Zotero/storage/49W26PC4/Rosu et al. - 2022 - Neural Strands Learning Hair Geometry and Appeara.pdf:application/pdf},
}

@misc{mildenhall_nerf_2020,
	title = {{NeRF}: Representing Scenes as Neural Radiance Fields for View Synthesis},
	url = {http://arxiv.org/abs/2003.08934},
	doi = {10.48550/arXiv.2003.08934},
	shorttitle = {{NeRF}},
	abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$({\textbackslash}theta, {\textbackslash}phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
	number = {{arXiv}:2003.08934},
	publisher = {{arXiv}},
	author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
	urldate = {2024-10-06},
	date = {2020-08-03},
	eprinttype = {arxiv},
	eprint = {2003.08934 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	file = {arXiv Fulltext PDF:/Users/fangjun/Zotero/storage/WRVG8FTW/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf:application/pdf;arXiv.org Snapshot:/Users/fangjun/Zotero/storage/MZHV8RUC/2003.html:text/html},
}

@misc{yang_neural_2023,
	title = {Neural Vector Fields: Implicit Representation by Explicit Learning},
	url = {http://arxiv.org/abs/2303.04341},
	shorttitle = {Neural Vector Fields},
	abstract = {Deep neural networks ({DNNs}) are widely applied for nowadays 3D surface reconstruction tasks and such methods can be further divided into two categories, which respectively warp templates explicitly by moving vertices or represent 3D surfaces implicitly as signed or unsigned distance functions. Taking advantage of both advanced explicit learning process and powerful representation ability of implicit functions, we propose a novel 3D representation method, Neural Vector Fields ({NVF}). It not only adopts the explicit learning process to manipulate meshes directly, but also leverages the implicit representation of unsigned distance functions ({UDFs}) to break the barriers in resolution and topology. Specifically, our method first predicts the displacements from queries towards the surface and models the shapes as Vector Fields. Rather than relying on network differentiation to obtain direction fields as most existing {UDF}-based methods, the produced vector fields encode the distance and direction fields both and mitigate the ambiguity at “ridge” points, such that the calculation of direction fields is straightforward and differentiation-free. The differentiation-free characteristic enables us to further learn a shape codebook via Vector Quantization, which encodes the cross-object priors, accelerates the training procedure, and boosts model generalization on cross-category reconstruction. The extensive experiments on surface reconstruction benchmarks indicate that our method outperforms those state-of-the-art methods in different evaluation scenarios including watertight vs nonwatertight shapes, category-specific vs category-agnostic reconstruction, category-unseen reconstruction, and crossdomain reconstruction. Our code is released at https: //github.com/Wi-sc/{NVF}.},
	number = {{arXiv}:2303.04341},
	publisher = {{arXiv}},
	author = {Yang, Xianghui and Lin, Guosheng and Chen, Zhenghao and Zhou, Luping},
	urldate = {2024-10-06},
	date = {2023-06-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2303.04341 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Graphics},
	file = {Yang et al. - 2023 - Neural Vector Fields Implicit Representation by E.pdf:/Users/fangjun/Zotero/storage/LB7TKNMF/Yang et al. - 2023 - Neural Vector Fields Implicit Representation by E.pdf:application/pdf},
}

@inproceedings{linjie_luo_multi-view_2012,
	location = {Providence, {RI}},
	title = {Multi-view hair capture using orientation fields},
	isbn = {978-1-4673-1228-8 978-1-4673-1226-4 978-1-4673-1227-1},
	url = {http://ieeexplore.ieee.org/document/6247838/},
	doi = {10.1109/CVPR.2012.6247838},
	abstract = {Reconstructing realistic 3D hair geometry is challenging due to omnipresent occlusions, complex discontinuities and specular appearance. To address these challenges, we propose a multi-view hair reconstruction algorithm based on orientation ﬁelds with structure-aware aggregation. Our key insight is that while hair’s color appearance is viewdependent, the response to oriented ﬁlters that captures the local hair orientation is more stable. We apply the structure-aware aggregation to the {MRF} matching energy to enforce the structural continuities implied from the local hair orientations. Multiple depth maps from the {MRF} optimization are then fused into a globally consistent hair geometry with a template reﬁnement procedure. Compared to the state-of-the-art color-based methods, our method faithfully reconstructs detailed hair structures. We demonstrate the results for a number of hair styles, ranging from straight to curly, and show that our framework is suitable for capturing hair in motion.},
	eventtitle = {2012 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {1490--1497},
	booktitle = {2012 {IEEE} Conference on Computer Vision and Pattern Recognition},
	publisher = {{IEEE}},
	author = {{Linjie Luo} and {Hao Li} and Paris, S. and Weise, T. and Pauly, M. and Rusinkiewicz, S.},
	urldate = {2024-10-06},
	date = {2012-06},
	langid = {english},
	file = {Linjie Luo et al. - 2012 - Multi-view hair capture using orientation fields.pdf:/Users/fangjun/Zotero/storage/F8CKMQ8B/Linjie Luo et al. - 2012 - Multi-view hair capture using orientation fields.pdf:application/pdf},
}

@article{nam_strand-accurate_nodate,
	title = {Strand-Accurate Multi-View Hair Capture},
	abstract = {Hair is one of the most challenging objects to reconstruct due to its micro-scale structure and a large number of repeated strands with heavy occlusions. In this paper, we present the ﬁrst method to capture high-ﬁdelity hair geometry with strand-level accuracy. Our method takes three stages to achieve this. In the ﬁrst stage, a new multi-view stereo method with a slanted support line is proposed to solve the hair correspondences between different views. In detail, we contribute a novel cost function consisting of both photo-consistency term and geometric term that reconstructs each hair pixel as a 3D line. By merging all the depth maps, a point cloud, as well as local line directions for each point, is obtained. Thus, in the second stage, we feature a novel strand reconstruction method with the mean-shift to convert the noisy point data to a set of strands. Lastly, we grow the hair strands with multi-view geometric constraints to elongate the short strands and recover the missing strands, thus signiﬁcantly increasing the reconstruction completeness. We evaluate our method on both synthetic data and real captured data, showing that our method can reconstruct hair strands with sub-millimeter accuracy.},
	author = {Nam, Giljoo and Wu, Chenglei and Kim, Min H and Sheikh, Yaser},
	langid = {english},
	file = {Nam et al. - Strand-Accurate Multi-View Hair Capture.pdf:/Users/fangjun/Zotero/storage/YVYV34Z3/Nam et al. - Strand-Accurate Multi-View Hair Capture.pdf:application/pdf},
}

@online{rouch_patchmatch_2023,
	title = {{PatchMatch} Multi-View Stereo},
	url = {https://betterprogramming.pub/patchmatch-multi-view-stereo-1-2-fc46e5dfe912},
	abstract = {Understand how the {MVS} tool provided by {COLMAP} works.},
	titleaddon = {Medium},
	author = {Rouch, Thomas},
	urldate = {2024-10-06},
	date = {2023-04-10},
	langid = {english},
}
