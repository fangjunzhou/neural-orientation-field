% $ biblatex auxiliary file $
% $ biblatex bbl format version 2.9 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nty/global//global/global}
    \entry{ma_single-view_nodate}{article}{}
      \name{author}{1}{}{%
        {{hash=2e999ccc73c7e59592e06040e5e1107b}{%
           family={Ma},
           familyi={M\bibinitperiod},
           given={Chongyang},
           giveni={C\bibinitperiod}}}%
      }
      \strng{namehash}{2e999ccc73c7e59592e06040e5e1107b}
      \strng{fullhash}{2e999ccc73c7e59592e06040e5e1107b}
      \strng{bibnamehash}{2e999ccc73c7e59592e06040e5e1107b}
      \strng{authorbibnamehash}{2e999ccc73c7e59592e06040e5e1107b}
      \strng{authornamehash}{2e999ccc73c7e59592e06040e5e1107b}
      \strng{authorfullhash}{2e999ccc73c7e59592e06040e5e1107b}
      \field{sortinit}{M}
      \field{sortinithash}{cfd219b90152c06204fab207bc6c7cab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Human hair presents highly convoluted structures and spans an extraordinarily wide range of hairstyles, which is essential for the digitization of compelling virtual avatars but also one of the most challenging to create. Cutting-edge hair modeling techniques typically rely on expensive capture devices and signi\x{fb01}cant manual labor. We introduce a novel data-driven framework that can digitize complete and highly complex 3D hairstyles from a single-view photograph. We \x{fb01}rst construct a large database of manually crafted hair models from several online repositories. Given a reference photo of the target hairstyle and a few user strokes as guidance, we automatically search for multiple best matching examples from the database and combine them consistently into a single hairstyle to form the large-scale structure of the hair model. We then synthesize the \x{fb01}nal hair strands by jointly optimizing for the projected 2D similarity to the reference photo, the physical plausibility of each strand, as well as the local orientation coherency between neighboring strands. We demonstrate the effectiveness and robustness of our method on a variety of hairstyles and challenging images, and compare our system with state-of-the-art hair modeling algorithms.}
      \field{langid}{english}
      \field{title}{Single-View Hair Modeling Using A Hairstyle Database}
      \verb{file}
      \verb Ma - Single-View Hair Modeling Using A Hairstyle Databa.pdf:/Users/fangjun/Zotero/storage/9BXWYQA5/Ma - Single-View Hair Modeling Using A Hairstyle Databa.pdf:application/pdf
      \endverb
    \endentry
    \entry{metzer_latent-nerf_2022}{misc}{}
      \name{author}{5}{}{%
        {{hash=4e211b87e927bda8e915e38340cb7daa}{%
           family={Metzer},
           familyi={M\bibinitperiod},
           given={Gal},
           giveni={G\bibinitperiod}}}%
        {{hash=89364e2db8972659a668cbdc092b95af}{%
           family={Richardson},
           familyi={R\bibinitperiod},
           given={Elad},
           giveni={E\bibinitperiod}}}%
        {{hash=6a8a6c7970ab6d02e599d62882543861}{%
           family={Patashnik},
           familyi={P\bibinitperiod},
           given={Or},
           giveni={O\bibinitperiod}}}%
        {{hash=d05a6442a67a9d44901f3a7688baed50}{%
           family={Giryes},
           familyi={G\bibinitperiod},
           given={Raja},
           giveni={R\bibinitperiod}}}%
        {{hash=cdf44d78713bc27a81394749a9d8bbdc}{%
           family={Cohen-Or},
           familyi={C\bibinithyphendelim O\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{61c6702d79abe79b4d634ae4efa3a557}
      \strng{fullhash}{2fcbc038344249f41846847abaf5f102}
      \strng{bibnamehash}{61c6702d79abe79b4d634ae4efa3a557}
      \strng{authorbibnamehash}{61c6702d79abe79b4d634ae4efa3a557}
      \strng{authornamehash}{61c6702d79abe79b4d634ae4efa3a557}
      \strng{authorfullhash}{2fcbc038344249f41846847abaf5f102}
      \field{sortinit}{M}
      \field{sortinithash}{cfd219b90152c06204fab207bc6c7cab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Text-guided image generation has progressed rapidly in recent years, inspiring major breakthroughs in text-guided shape generation. Recently, it has been shown that using score distillation, one can successfully text-guide a {NeRF} model to generate a 3D object. We adapt the score distillation to the publicly available, and computationally efficient, Latent Diffusion Models, which apply the entire diffusion process in a compact latent space of a pretrained autoencoder. As {NeRFs} operate in image space, a naive solution for guiding them with latent score distillation would require encoding to the latent space at each guidance step. Instead, we propose to bring the {NeRF} to the latent space, resulting in a Latent-{NeRF}. Analyzing our Latent-{NeRF}, we show that while Text-to-3D models can generate impressive results, they are inherently unconstrained and may lack the ability to guide or enforce a specific 3D structure. To assist and direct the 3D generation, we propose to guide our Latent-{NeRF} using a Sketch-Shape: an abstract geometry that defines the coarse structure of the desired object. Then, we present means to integrate such a constraint directly into a Latent-{NeRF}. This unique combination of text and shape guidance allows for increased control over the generation process. We also show that latent score distillation can be successfully applied directly on 3D meshes. This allows for generating high-quality textures on a given geometry. Our experiments validate the power of our different forms of guidance and the efficiency of using latent rendering. Implementation is available at https://github.com/eladrich/latent-nerf}
      \field{day}{14}
      \field{eprinttype}{arxiv}
      \field{month}{11}
      \field{number}{{arXiv}:2211.07600}
      \field{title}{Latent-{NeRF} for Shape-Guided Generation of 3D Shapes and Textures}
      \field{urlday}{9}
      \field{urlmonth}{11}
      \field{urlyear}{2024}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{eprint}
      \verb 2211.07600
      \endverb
      \verb{file}
      \verb Full Text PDF:/Users/fangjun/Zotero/storage/3MY5TLI5/Metzer et al. - 2022 - Latent-NeRF for Shape-Guided Generation of 3D Shap.pdf:application/pdf;Snapshot:/Users/fangjun/Zotero/storage/Z6ILBNS2/2211.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2211.07600
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2211.07600
      \endverb
      \keyw{Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics}
    \endentry
    \entry{mildenhall_nerf_2020}{misc}{}
      \name{author}{6}{}{%
        {{hash=a3567364ab52973659adf74ee0c6e6f3}{%
           family={Mildenhall},
           familyi={M\bibinitperiod},
           given={Ben},
           giveni={B\bibinitperiod}}}%
        {{hash=2d8bed08e2e866c07b46f1258895e6a5}{%
           family={Srinivasan},
           familyi={S\bibinitperiod},
           given={Pratul\bibnamedelima P.},
           giveni={P\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=9a18985a5c8b8be23d0ec9de0a3c87b9}{%
           family={Tancik},
           familyi={T\bibinitperiod},
           given={Matthew},
           giveni={M\bibinitperiod}}}%
        {{hash=4ef9d5fb0962f22456e4c8ea6910659a}{%
           family={Barron},
           familyi={B\bibinitperiod},
           given={Jonathan\bibnamedelima T.},
           giveni={J\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
        {{hash=e7e274981ceb0dd123d44af35cc7bedf}{%
           family={Ramamoorthi},
           familyi={R\bibinitperiod},
           given={Ravi},
           giveni={R\bibinitperiod}}}%
        {{hash=96e30bdc2cf905aa1109c46617b16252}{%
           family={Ng},
           familyi={N\bibinitperiod},
           given={Ren},
           giveni={R\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{094342ecc1d4c7df1a909882dd0a89cd}
      \strng{fullhash}{5df0df9265ea6976fca3ea466992dede}
      \strng{bibnamehash}{094342ecc1d4c7df1a909882dd0a89cd}
      \strng{authorbibnamehash}{094342ecc1d4c7df1a909882dd0a89cd}
      \strng{authornamehash}{094342ecc1d4c7df1a909882dd0a89cd}
      \strng{authorfullhash}{5df0df9265ea6976fca3ea466992dede}
      \field{sortinit}{M}
      \field{sortinithash}{cfd219b90152c06204fab207bc6c7cab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$({\textbackslash}theta, {\textbackslash}phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.}
      \field{day}{3}
      \field{eprinttype}{arxiv}
      \field{month}{8}
      \field{number}{{arXiv}:2003.08934}
      \field{shorttitle}{{NeRF}}
      \field{title}{{NeRF}: Representing Scenes as Neural Radiance Fields for View Synthesis}
      \field{urlday}{6}
      \field{urlmonth}{10}
      \field{urlyear}{2024}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2003.08934
      \endverb
      \verb{eprint}
      \verb 2003.08934 [cs]
      \endverb
      \verb{file}
      \verb arXiv Fulltext PDF:/Users/fangjun/Zotero/storage/WRVG8FTW/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf:application/pdf;arXiv.org Snapshot:/Users/fangjun/Zotero/storage/MZHV8RUC/2003.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2003.08934
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2003.08934
      \endverb
      \keyw{Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics}
    \endentry
    \entry{nam_strand-accurate_nodate}{article}{}
      \name{author}{4}{}{%
        {{hash=b436258dc0e62b689d425b9684271c4a}{%
           family={Nam},
           familyi={N\bibinitperiod},
           given={Giljoo},
           giveni={G\bibinitperiod}}}%
        {{hash=86268fde709e065d68d7242b80eba98c}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Chenglei},
           giveni={C\bibinitperiod}}}%
        {{hash=dc3bbe4adad2d011add88df3cdbbd141}{%
           family={Kim},
           familyi={K\bibinitperiod},
           given={Min\bibnamedelima H},
           giveni={M\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
        {{hash=eec556189169ec6c7325cc7a804cbc02}{%
           family={Sheikh},
           familyi={S\bibinitperiod},
           given={Yaser},
           giveni={Y\bibinitperiod}}}%
      }
      \strng{namehash}{2140fd5fde79bf93f7c8e436919d5a24}
      \strng{fullhash}{f6043eb252c94ed8b43888fd59e4d358}
      \strng{bibnamehash}{2140fd5fde79bf93f7c8e436919d5a24}
      \strng{authorbibnamehash}{2140fd5fde79bf93f7c8e436919d5a24}
      \strng{authornamehash}{2140fd5fde79bf93f7c8e436919d5a24}
      \strng{authorfullhash}{f6043eb252c94ed8b43888fd59e4d358}
      \field{sortinit}{N}
      \field{sortinithash}{f7242c3ed3dc50029fca1be76c497c7c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Hair is one of the most challenging objects to reconstruct due to its micro-scale structure and a large number of repeated strands with heavy occlusions. In this paper, we present the \x{fb01}rst method to capture high-\x{fb01}delity hair geometry with strand-level accuracy. Our method takes three stages to achieve this. In the \x{fb01}rst stage, a new multi-view stereo method with a slanted support line is proposed to solve the hair correspondences between different views. In detail, we contribute a novel cost function consisting of both photo-consistency term and geometric term that reconstructs each hair pixel as a 3D line. By merging all the depth maps, a point cloud, as well as local line directions for each point, is obtained. Thus, in the second stage, we feature a novel strand reconstruction method with the mean-shift to convert the noisy point data to a set of strands. Lastly, we grow the hair strands with multi-view geometric constraints to elongate the short strands and recover the missing strands, thus signi\x{fb01}cantly increasing the reconstruction completeness. We evaluate our method on both synthetic data and real captured data, showing that our method can reconstruct hair strands with sub-millimeter accuracy.}
      \field{langid}{english}
      \field{title}{Strand-Accurate Multi-View Hair Capture}
      \verb{file}
      \verb Nam et al. - Strand-Accurate Multi-View Hair Capture.pdf:/Users/fangjun/Zotero/storage/YVYV34Z3/Nam et al. - Strand-Accurate Multi-View Hair Capture.pdf:application/pdf
      \endverb
    \endentry
    \entry{rosu_neural_2022}{misc}{}
      \name{author}{6}{}{%
        {{hash=f7741d9bc32958d3e21284bb63eaac8c}{%
           family={Rosu},
           familyi={R\bibinitperiod},
           given={Radu\bibnamedelima Alexandru},
           giveni={R\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=22eec8f6b54b199bae783d56e79beca9}{%
           family={Saito},
           familyi={S\bibinitperiod},
           given={Shunsuke},
           giveni={S\bibinitperiod}}}%
        {{hash=1592421333bfd0f0959df5c10c89b491}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Ziyan},
           giveni={Z\bibinitperiod}}}%
        {{hash=86268fde709e065d68d7242b80eba98c}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Chenglei},
           giveni={C\bibinitperiod}}}%
        {{hash=3110caa22b682a3c7f48017d49dccc6b}{%
           family={Behnke},
           familyi={B\bibinitperiod},
           given={Sven},
           giveni={S\bibinitperiod}}}%
        {{hash=b436258dc0e62b689d425b9684271c4a}{%
           family={Nam},
           familyi={N\bibinitperiod},
           given={Giljoo},
           giveni={G\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{b948df6dc6975649dd87765471e8e0f2}
      \strng{fullhash}{9b27c0dd5581e82bdaafb5092e1e2812}
      \strng{bibnamehash}{b948df6dc6975649dd87765471e8e0f2}
      \strng{authorbibnamehash}{b948df6dc6975649dd87765471e8e0f2}
      \strng{authornamehash}{b948df6dc6975649dd87765471e8e0f2}
      \strng{authorfullhash}{9b27c0dd5581e82bdaafb5092e1e2812}
      \field{sortinit}{R}
      \field{sortinithash}{da6b42bd3ab22fee61abed031ee405f7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We present Neural Strands, a novel learning framework for modeling accurate hair geometry and appearance from multi-view image inputs. The learned hair model can be rendered in real-time from any viewpoint with high-fidelity view-dependent effects. Our model achieves intuitive shape and style control unlike volumetric counterparts. To enable these properties, we propose a novel hair representation based on a neural scalp texture that encodes the geometry and appearance of individual strands at each texel location. Furthermore, we introduce a novel neural rendering framework based on rasterization of the learned hair strands. Our neural rendering is strand-accurate and anti-aliased, making the rendering view-consistent and photorealistic. Combining appearance with a multi-view geometric prior, we enable, for the first time, the joint learning of appearance and explicit hair geometry from a multi-view setup. We demonstrate the efficacy of our approach in terms of fidelity and efficiency for various hairstyles.}
      \field{day}{28}
      \field{eprinttype}{arxiv}
      \field{langid}{english}
      \field{month}{7}
      \field{number}{{arXiv}:2207.14067}
      \field{shorttitle}{Neural Strands}
      \field{title}{Neural Strands: Learning Hair Geometry and Appearance from Multi-View Images}
      \field{urlday}{6}
      \field{urlmonth}{10}
      \field{urlyear}{2024}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{eprint}
      \verb 2207.14067 [cs]
      \endverb
      \verb{file}
      \verb Rosu et al. - 2022 - Neural Strands Learning Hair Geometry and Appeara.pdf:/Users/fangjun/Zotero/storage/49W26PC4/Rosu et al. - 2022 - Neural Strands Learning Hair Geometry and Appeara.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2207.14067
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2207.14067
      \endverb
      \keyw{Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics}
    \endentry
    \entry{saito_3d_2018}{article}{}
      \name{author}{6}{}{%
        {{hash=22eec8f6b54b199bae783d56e79beca9}{%
           family={Saito},
           familyi={S\bibinitperiod},
           given={Shunsuke},
           giveni={S\bibinitperiod}}}%
        {{hash=c0287821ffc85688789efccb54a5ca4f}{%
           family={Hu},
           familyi={H\bibinitperiod},
           given={Liwen},
           giveni={L\bibinitperiod}}}%
        {{hash=2e999ccc73c7e59592e06040e5e1107b}{%
           family={Ma},
           familyi={M\bibinitperiod},
           given={Chongyang},
           giveni={C\bibinitperiod}}}%
        {{hash=5ccb287cfec77012fbee06549ce84cda}{%
           family={Ibayashi},
           familyi={I\bibinitperiod},
           given={Hikaru},
           giveni={H\bibinitperiod}}}%
        {{hash=8a195ee977989f3d0e88c6987974bbc0}{%
           family={Luo},
           familyi={L\bibinitperiod},
           given={Linjie},
           giveni={L\bibinitperiod}}}%
        {{hash=2620b9afd37cca5b9b7354f67c036d4d}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Hao},
           giveni={H\bibinitperiod}}}%
      }
      \strng{namehash}{fcb9ed319b108775c91fd34ec852f0b1}
      \strng{fullhash}{5fe0923a3b62c4a3e3a87ed329b0b68d}
      \strng{bibnamehash}{fcb9ed319b108775c91fd34ec852f0b1}
      \strng{authorbibnamehash}{fcb9ed319b108775c91fd34ec852f0b1}
      \strng{authornamehash}{fcb9ed319b108775c91fd34ec852f0b1}
      \strng{authorfullhash}{5fe0923a3b62c4a3e3a87ed329b0b68d}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recent advances in single-view 3D hair digitization have made the creation of high-quality {CG} characters scalable and accessible to end-users, enabling new forms of personalized {VR} and gaming experiences. To handle the complexity and variety of hair structures, most cutting-edge techniques rely on the successful retrieval of a particular hair model from a comprehensive hair database. Not only are the aforementioned data-driven methods storage intensive, but they are also prone to failure for highly unconstrained input images, complicated hairstyles, and failed face detection. Instead of using a large collection of 3D hair models directly, we propose to represent the manifold of 3D hairstyles implicitly through a compact latent space of a volumetric variational autoencoder ({VAE}). This deep neural network is trained with volumetric orientation field representations of 3D hair models and can synthesize new hairstyles from a compressed code. To enable end-to-end 3D hair inference, we train an additional embedding network to predict the code in the {VAE} latent space from any input image. Strand-level hairstyles can then be generated from the predicted volumetric representation. Our fully automatic framework does not require any ad-hoc face fitting, intermediate classification and segmentation, or hairstyle database retrieval. Our hair synthesis approach is significantly more robust and can handle a much wider variation of hairstyles than state-of-the-art data-driven hair modeling techniques with challenging inputs, including photos that are low-resolution, overexposured, or contain extreme head poses. The storage requirements are minimal and a 3D hair model can be produced from an image in a second. Our evaluations also show that successful reconstructions are possible from highly stylized cartoon images, non-human subjects, and pictures taken from behind a person. Our approach is particularly well suited for continuous and plausible hair interpolation between very different hairstyles.}
      \field{day}{31}
      \field{issn}{0730-0301, 1557-7368}
      \field{journaltitle}{{ACM} Transactions on Graphics}
      \field{langid}{english}
      \field{month}{12}
      \field{number}{6}
      \field{shortjournal}{{ACM} Trans. Graph.}
      \field{title}{3D hair synthesis using volumetric variational autoencoders}
      \field{urlday}{6}
      \field{urlmonth}{10}
      \field{urlyear}{2024}
      \field{volume}{37}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 12}
      \range{pages}{12}
      \verb{doi}
      \verb 10.1145/3272127.3275019
      \endverb
      \verb{file}
      \verb Saito et al. - 2018 - 3D hair synthesis using volumetric variational aut.pdf:/Users/fangjun/Zotero/storage/W8QESIHG/Saito et al. - 2018 - 3D hair synthesis using volumetric variational aut.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://dl.acm.org/doi/10.1145/3272127.3275019
      \endverb
      \verb{url}
      \verb https://dl.acm.org/doi/10.1145/3272127.3275019
      \endverb
    \endentry
    \entry{sklyarova_neural_2023}{misc}{}
      \name{author}{6}{}{%
        {{hash=b2e5188a72ec0827d7facac468041ad5}{%
           family={Sklyarova},
           familyi={S\bibinitperiod},
           given={Vanessa},
           giveni={V\bibinitperiod}}}%
        {{hash=8dc3dccd346756c0bd8426b026b69029}{%
           family={Chelishev},
           familyi={C\bibinitperiod},
           given={Jenya},
           giveni={J\bibinitperiod}}}%
        {{hash=16fd2360346e15343bc5197d338961b7}{%
           family={Dogaru},
           familyi={D\bibinitperiod},
           given={Andreea},
           giveni={A\bibinitperiod}}}%
        {{hash=19877d6ecfa8f1f6112d8df4e7b0b9ec}{%
           family={Medvedev},
           familyi={M\bibinitperiod},
           given={Igor},
           giveni={I\bibinitperiod}}}%
        {{hash=a48f60762c88685a899455e12615e1df}{%
           family={Lempitsky},
           familyi={L\bibinitperiod},
           given={Victor},
           giveni={V\bibinitperiod}}}%
        {{hash=338eac6a6e80df412c00481a190ae918}{%
           family={Zakharov},
           familyi={Z\bibinitperiod},
           given={Egor},
           giveni={E\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{7c088f4f987ebed3f526195c7c6d99bb}
      \strng{fullhash}{0207915072526d44acd1759d9b4abd01}
      \strng{bibnamehash}{7c088f4f987ebed3f526195c7c6d99bb}
      \strng{authorbibnamehash}{7c088f4f987ebed3f526195c7c6d99bb}
      \strng{authornamehash}{7c088f4f987ebed3f526195c7c6d99bb}
      \strng{authorfullhash}{0207915072526d44acd1759d9b4abd01}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Generating realistic human 3D reconstructions using image or video data is essential for various communication and entertainment applications. While existing methods achieved impressive results for body and facial regions, realistic hair modeling still remains challenging due to its high mechanical complexity. This work proposes an approach capable of accurate hair geometry reconstruction at a strand level from a monocular video or multi-view images captured in uncontrolled lighting conditions. Our method has two stages, with the first stage performing joint reconstruction of coarse hair and bust shapes and hair orientation using implicit volumetric representations. The second stage then estimates a strand-level hair reconstruction by reconciling in a single optimization process the coarse volumetric constraints with hair strand and hairstyle priors learned from the synthetic data. To further increase the reconstruction fidelity, we incorporate image-based losses into the fitting process using a new differentiable renderer. The combined system, named Neural Haircut, achieves high realism and personalization of the reconstructed hairstyles.}
      \field{day}{12}
      \field{eprinttype}{arxiv}
      \field{month}{6}
      \field{number}{{arXiv}:2306.05872}
      \field{shorttitle}{Neural Haircut}
      \field{title}{Neural Haircut: Prior-Guided Strand-Based Hair Reconstruction}
      \field{urlday}{6}
      \field{urlmonth}{10}
      \field{urlyear}{2024}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2306.05872
      \endverb
      \verb{eprint}
      \verb 2306.05872 [cs]
      \endverb
      \verb{file}
      \verb arXiv Fulltext PDF:/Users/fangjun/Zotero/storage/5KZSJMTK/Sklyarova et al. - 2023 - Neural Haircut Prior-Guided Strand-Based Hair Rec.pdf:application/pdf;arXiv.org Snapshot:/Users/fangjun/Zotero/storage/4F38CUKM/2306.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2306.05872
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2306.05872
      \endverb
      \keyw{Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics}
    \endentry
    \entry{wu_neuralhdhair_2022}{inproceedings}{}
      \name{author}{6}{}{%
        {{hash=48315c800ddd486d8b0d21a9302b92d1}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Keyu},
           giveni={K\bibinitperiod}}}%
        {{hash=50b0fe08684e0077abf742955cd7b53f}{%
           family={Ye},
           familyi={Y\bibinitperiod},
           given={Yifan},
           giveni={Y\bibinitperiod}}}%
        {{hash=163d4d7e9641ec57bc1ec4aa1ced6647}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Lingchen},
           giveni={L\bibinitperiod}}}%
        {{hash=8d1e028082ee746f233ba6b3ae403928}{%
           family={Fu},
           familyi={F\bibinitperiod},
           given={Hongbo},
           giveni={H\bibinitperiod}}}%
        {{hash=09d5d500b34786c94a128e6f131300c6}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Kun},
           giveni={K\bibinitperiod}}}%
        {{hash=e0168e300f0e0260338a499e401a6dc5}{%
           family={Zhengl},
           familyi={Z\bibinitperiod},
           given={Youyi},
           giveni={Y\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {New Orleans, {LA}, {USA}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{6ac794a42ec77fd49662a092be00fc3f}
      \strng{fullhash}{a495ea499acdf09f7c09679931035673}
      \strng{bibnamehash}{6ac794a42ec77fd49662a092be00fc3f}
      \strng{authorbibnamehash}{6ac794a42ec77fd49662a092be00fc3f}
      \strng{authornamehash}{6ac794a42ec77fd49662a092be00fc3f}
      \strng{authorfullhash}{a495ea499acdf09f7c09679931035673}
      \field{sortinit}{W}
      \field{sortinithash}{ecb89ff85896a47dc313960773ac311d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Undoubtedly, high-\x{fb01}delity 3D hair plays an indispensable role in digital humans. However, existing monocular hair modeling methods are either tricky to deploy in digital systems (e.g., due to their dependence on complex user interactions or large databases) or can produce only a coarse geometry. In this paper, we introduce {NeuralHDHair}, a \x{fb02}exible, fully automatic system for modeling high-\x{fb01}delity hair from a single image. The key enablers of our system are two carefully designed neural networks: an {IRHairNet} (Implicit representation for hair using neural network) for inferring high-\x{fb01}delity 3D hair geometric features (3D orientation \x{fb01}eld and 3D occupancy \x{fb01}eld) hierarchically and a {GrowingNet} (Growing hair strands using neural network) to ef\x{fb01}ciently generate 3D hair strands in parallel. Specifically, we perform a coarse-to-\x{fb01}ne manner and propose a novel voxel-aligned implicit function ({VIFu}) to represent the global hair feature, which is further enhanced by the local details extracted from a hair luminance map. To improve the ef\x{fb01}ciency of a traditional hair growth algorithm, we adopt a local neural implicit function to grow strands based on the estimated 3D hair geometric features. Extensive experiments show that our method is capable of constructing a high-\x{fb01}delity 3D hair model from a single image, both ef\x{fb01}ciently and effectively, and achieves the-state-of-the-art performance.}
      \field{booktitle}{2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})}
      \field{eventtitle}{2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})}
      \field{isbn}{978-1-66546-946-3}
      \field{langid}{english}
      \field{month}{6}
      \field{shorttitle}{{NeuralHDHair}}
      \field{title}{{NeuralHDHair}: Automatic High-fidelity Hair Modeling from a Single Image Using Implicit Neural Representations}
      \field{urlday}{6}
      \field{urlmonth}{10}
      \field{urlyear}{2024}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1516\bibrangedash 1525}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1109/CVPR52688.2022.00158
      \endverb
      \verb{file}
      \verb Wu et al. - 2022 - NeuralHDHair Automatic High-fidelity Hair Modelin.pdf:/Users/fangjun/Zotero/storage/BXH4YN45/Wu et al. - 2022 - NeuralHDHair Automatic High-fidelity Hair Modelin.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/9878513/
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/9878513/
      \endverb
    \endentry
    \entry{zheng_hairstep_2023}{misc}{}
      \name{author}{7}{}{%
        {{hash=0c12dcfe941f3b29f2d2d9702348d69e}{%
           family={Zheng},
           familyi={Z\bibinitperiod},
           given={Yujian},
           giveni={Y\bibinitperiod}}}%
        {{hash=452e2b5aa94e8c9d89755da8dacb6d1a}{%
           family={Jin},
           familyi={J\bibinitperiod},
           given={Zirong},
           giveni={Z\bibinitperiod}}}%
        {{hash=d6ef042f1c26c6587dc2cbd0c8be3206}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Moran},
           giveni={M\bibinitperiod}}}%
        {{hash=c5a7aefdcbfaee77f0a55e247610f28b}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Haibin},
           giveni={H\bibinitperiod}}}%
        {{hash=2e999ccc73c7e59592e06040e5e1107b}{%
           family={Ma},
           familyi={M\bibinitperiod},
           given={Chongyang},
           giveni={C\bibinitperiod}}}%
        {{hash=6eb52b287f341af394b7385d519efb5b}{%
           family={Cui},
           familyi={C\bibinitperiod},
           given={Shuguang},
           giveni={S\bibinitperiod}}}%
        {{hash=ce3682554f147fbd9c76c566cf0ec373}{%
           family={Han},
           familyi={H\bibinitperiod},
           given={Xiaoguang},
           giveni={X\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{4a681323a95b8e2304401e120abf82e3}
      \strng{fullhash}{f544cf7768d41234a84405c179a7aa8d}
      \strng{bibnamehash}{4a681323a95b8e2304401e120abf82e3}
      \strng{authorbibnamehash}{4a681323a95b8e2304401e120abf82e3}
      \strng{authornamehash}{4a681323a95b8e2304401e120abf82e3}
      \strng{authorfullhash}{f544cf7768d41234a84405c179a7aa8d}
      \field{sortinit}{Z}
      \field{sortinithash}{156173bd08b075d7295bc3e0f4735a04}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{In this work, we tackle the challenging problem of learning-based single-view 3D hair modeling. Due to the great difficulty of collecting paired real image and 3D hair data, using synthetic data to provide prior knowledge for real domain becomes a leading solution. This unfortunately introduces the challenge of domain gap. Due to the inherent difficulty of realistic hair rendering, existing methods typically use orientation maps instead of hair images as input to bridge the gap. We firmly think an intermediate representation is essential, but we argue that orientation map using the dominant filtering-based methods is sensitive to uncertain noise and far from a competent representation. Thus, we first raise this issue up and propose a novel intermediate representation, termed as {HairStep}, which consists of a strand map and a depth map. It is found that {HairStep} not only provides sufficient information for accurate 3D hair modeling, but also is feasible to be inferred from real images. Specifically, we collect a dataset of 1,250 portrait images with two types of annotations. A learning framework is further designed to transfer real images to the strand map and depth map. It is noted that, an extra bonus of our new dataset is the first quantitative metric for 3D hair modeling. Our experiments show that {HairStep} narrows the domain gap between synthetic and real and achieves state-of-the-art performance on single-view 3D hair reconstruction.}
      \field{day}{23}
      \field{eprinttype}{arxiv}
      \field{month}{3}
      \field{number}{{arXiv}:2303.02700}
      \field{shorttitle}{{HairStep}}
      \field{title}{{HairStep}: Transfer Synthetic to Real Using Strand and Depth Maps for Single-View 3D Hair Modeling}
      \field{urlday}{6}
      \field{urlmonth}{10}
      \field{urlyear}{2024}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2303.02700
      \endverb
      \verb{eprint}
      \verb 2303.02700 [cs]
      \endverb
      \verb{file}
      \verb arXiv Fulltext PDF:/Users/fangjun/Zotero/storage/QGMGHNKZ/Zheng et al. - 2023 - HairStep Transfer Synthetic to Real Using Strand .pdf:application/pdf;arXiv.org Snapshot:/Users/fangjun/Zotero/storage/HTQFEICE/2303.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2303.02700
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2303.02700
      \endverb
      \keyw{Computer Science - Computer Vision and Pattern Recognition}
    \endentry
  \enddatalist
\endrefsection
\endinput

