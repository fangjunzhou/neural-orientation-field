
@misc{zheng_hairstep_2023,
	title = {{HairStep}: Transfer Synthetic to Real Using Strand and Depth Maps for Single-View 3D Hair Modeling},
	url = {http://arxiv.org/abs/2303.02700},
	doi = {10.48550/arXiv.2303.02700},
	shorttitle = {{HairStep}},
	abstract = {In this work, we tackle the challenging problem of learning-based single-view 3D hair modeling. Due to the great difficulty of collecting paired real image and 3D hair data, using synthetic data to provide prior knowledge for real domain becomes a leading solution. This unfortunately introduces the challenge of domain gap. Due to the inherent difficulty of realistic hair rendering, existing methods typically use orientation maps instead of hair images as input to bridge the gap. We firmly think an intermediate representation is essential, but we argue that orientation map using the dominant filtering-based methods is sensitive to uncertain noise and far from a competent representation. Thus, we first raise this issue up and propose a novel intermediate representation, termed as {HairStep}, which consists of a strand map and a depth map. It is found that {HairStep} not only provides sufficient information for accurate 3D hair modeling, but also is feasible to be inferred from real images. Specifically, we collect a dataset of 1,250 portrait images with two types of annotations. A learning framework is further designed to transfer real images to the strand map and depth map. It is noted that, an extra bonus of our new dataset is the first quantitative metric for 3D hair modeling. Our experiments show that {HairStep} narrows the domain gap between synthetic and real and achieves state-of-the-art performance on single-view 3D hair reconstruction.},
	number = {{arXiv}:2303.02700},
	publisher = {{arXiv}},
	author = {Zheng, Yujian and Jin, Zirong and Li, Moran and Huang, Haibin and Ma, Chongyang and Cui, Shuguang and Han, Xiaoguang},
	urldate = {2024-10-06},
	date = {2023-03-23},
	eprinttype = {arxiv},
	eprint = {2303.02700 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/fangjun/Zotero/storage/QGMGHNKZ/Zheng et al. - 2023 - HairStep Transfer Synthetic to Real Using Strand .pdf:application/pdf;arXiv.org Snapshot:/Users/fangjun/Zotero/storage/HTQFEICE/2303.html:text/html},
}


@misc{mildenhall_nerf_2020,
	title = {{NeRF}: Representing Scenes as Neural Radiance Fields for View Synthesis},
	url = {http://arxiv.org/abs/2003.08934},
	doi = {10.48550/arXiv.2003.08934},
	shorttitle = {{NeRF}},
	abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$({\textbackslash}theta, {\textbackslash}phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
	number = {{arXiv}:2003.08934},
	publisher = {{arXiv}},
	author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
	urldate = {2024-10-06},
	date = {2020-08-03},
	eprinttype = {arxiv},
	eprint = {2003.08934 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	file = {arXiv Fulltext PDF:/Users/fangjun/Zotero/storage/WRVG8FTW/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf:application/pdf;arXiv.org Snapshot:/Users/fangjun/Zotero/storage/MZHV8RUC/2003.html:text/html},
}
